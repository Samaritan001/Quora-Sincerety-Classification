{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-gfUT5SGVhh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_default_dtype(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5UiW1vXKnd7",
        "outputId": "e32409b7-da52-4b80-e1f0-97d0e76932ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct  6 09:53:13 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EBKGs2zLPEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UTh9sQbKqxX",
        "outputId": "7470f068-d0bd-4074-ef60-15ecd99e6a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "Embedding_Dim = 100\n",
        "Hidden_Dim = 256\n",
        "Num_Layers = 1\n",
        "Bidirectional = True\n",
        "Dropout = 0.0\n",
        "Batch_Size = 1\n",
        "Epochs = 100\n",
        "N_Eval = 100\n",
        "vocab_size = 2116+2\n",
        "# PRATEIK = 1011110111110001000010\n",
        "# JEFFREY = \"RmVicnVyYXJ5IDFzdCwgMjAwMw==\"\n",
        "\n",
        "hyperparameters = {\"embedding_dim\": Embedding_Dim, \"hidden_dim\": Hidden_Dim,\n",
        "                   \"num_layers\": Num_Layers, \"bidirectional\": Bidirectional,\n",
        "                   \"dropout\": Dropout, \"batch_size\": Batch_Size,\n",
        "                   \"epochs\": Epochs}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "path = (\"train.csv\")"
      ],
      "metadata": {
        "id": "gqrVAjiG2reY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvDJ0tfkQoSU",
        "outputId": "0812d1f0-5977-4d1f-f48c-df48ff10d0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "z_xgykdHtx8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class StartingDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Bag of Words Dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: dataset constructor.\n",
        "    def __init__(self, data_path, device, data_size=-1):\n",
        "        '''\n",
        "        data_path (str): path for the csv file that contains the data that you want to use\n",
        "        '''\n",
        "\n",
        "        # Preprocess the data. These are just library function calls so it's here for you\n",
        "        self.df = pd.read_csv(data_path)\n",
        "        self.vectorizer = CountVectorizer(stop_words='english', max_df=0.995, min_df=0.0005)\n",
        "        #self.vectorizer = CountVectorizer(stop_words='english', max_df=1, min_df=0)\n",
        "        if data_size < 0:\n",
        "            data_size= len(self.df)\n",
        "        self.sequences = self.vectorizer.fit_transform(self.df.question_text.tolist()[:data_size]) # matrix of word counts for each sample\n",
        "        self.labels = torch.tensor(self.df.target.tolist(), dtype=torch.float32).to(device) # labels\n",
        "        self.token2idx = self.vectorizer.vocabulary_ # dictionary converting words to their counts\n",
        "        self.idx2token = {idx: token for token, idx in self.token2idx.items()} # same dictionary backwards\n",
        "\n",
        "        self.sentence_lengths = []\n",
        "        self.idxdf = []\n",
        "        for i in range(data_size):\n",
        "            words = re.split(r'[. ,!?;]', self.df.question_text[i])\n",
        "            indices = []\n",
        "            for word in words:\n",
        "                if word:\n",
        "                    if self.token2idx.get(word, False):\n",
        "                        indices.append(self.token2idx[word]+2)\n",
        "                    else:\n",
        "                        indices.append(1)\n",
        "            self.sentence_lengths.append(len(indices))\n",
        "            self.idxdf.append(torch.tensor(indices))\n",
        "\n",
        "        self.sentence_lengths = torch.tensor(self.sentence_lengths).to(device)\n",
        "        self.padded = pad_sequence(self.idxdf, batch_first=True).to(device)\n",
        "        # self.packed_input = pack_padded_sequence(padded, self.sentence_lengths, batch_first=True, enforce_sorted=False)\n",
        "        # self.idxdf = torch.tensor(self.idxdf).to(device)\n",
        "        # torch.tensor([[self.token2idx[word] if word for word in re.split(r'[. ,!?;]', self.df.question_text[i])] for i in range(size)])\n",
        "\n",
        "\n",
        "    # TODO: return an instance from the dataset\n",
        "    def __getitem__(self, i):\n",
        "        '''\n",
        "        i (int): the desired instance of the dataset\n",
        "        '''\n",
        "        '''\n",
        "        # return the ith sample's list of word counts and label\n",
        "        features = np.float32(self.sequences[i, :].toarray())\n",
        "        # features = self.sequences[i, :].toarray()\n",
        "        labels = self.labels[i]\n",
        "        labels = np.float32(labels)\n",
        "        return features, labels\n",
        "        '''\n",
        "\n",
        "        # data is [padded input, sentence length]\n",
        "        # label is label\n",
        "        return [self.padded[i], self.sentence_lengths[i]], self.labels[i]\n",
        "\n",
        "\n",
        "    # TODO: return the size of the dataset\n",
        "    def __len__(self):\n",
        "        return self.sequences.shape[0]"
      ],
      "metadata": {
        "id": "f70wp-4oKpBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StartingDataset(path, device, data_size=50)"
      ],
      "metadata": {
        "id": "7r1JCtkLfVTS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "7b6df114-c76c-4ac4-8393-f0a72f76fa1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6ce607828c4e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStartingDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-9c4e019de67f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, device, data_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Preprocess the data. These are just library function calls so it's here for you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.995\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#self.vectorizer = CountVectorizer(stop_words='english', max_df=1, min_df=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset.token2idx))\n",
        "print(dataset.padded)\n",
        "print(dataset.padded.size(0), dataset.padded.size(1))\n",
        "print(len(dataset.token2idx))\n",
        "print(len(dataset.sentence_lengths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyM3-UMkb7KX",
        "outputId": "bb71c039-a55d-4171-ebd8-f2d1ce498ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "299\n",
            "tensor([[  1,  69,   1,  ...,   0,   0,   0],\n",
            "        [  1,   1,   1,  ...,   0,   0,   0],\n",
            "        [  1,  75, 279,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  1,   1,   1,  ...,   0,   0,   0],\n",
            "        [  1,   1,   1,  ...,   0,   0,   0],\n",
            "        [  1,   1,   1,  ...,   0,   0,   0]])\n",
            "299 44\n",
            "299\n",
            "299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset.df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5us1q_3FVRe",
        "outputId": "b1f55efb-542b-4423-b447-c7f219a02a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1306122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test pad, pack, and embedding process\n",
        "\n",
        "embedding_layer = nn.Embedding(len(dataset.token2idx)+2, 30)\n",
        "embedded = embedding_layer(dataset.padded)\n",
        "print(embedded)\n",
        "packed_input = pack_padded_sequence(embedded, dataset.sentence_lengths, batch_first=True, enforce_sorted=False)\n",
        "print(packed_input)\n",
        "\n",
        "lstm_layer = nn.LSTM(input_size=30, hidden_size=10, batch_first=True)\n",
        "packed_output, (h_n, c_n) = lstm_layer(packed_input)\n",
        "#print(packed_output)\n",
        "output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDxH1uOp3BAX",
        "outputId": "669c838f-e2f3-41dc-ccf3-205f0bf67ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.5836,  0.6309,  0.6375,  ..., -0.0859, -1.0741,  2.7430],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         ...,\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505]],\n",
            "\n",
            "        [[ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         ...,\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505]],\n",
            "\n",
            "        [[ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [-0.4328, -0.8398,  0.3714,  ...,  1.9002, -0.6433, -0.4558],\n",
            "         [-0.1854, -1.1774, -0.0361,  ..., -0.7249,  0.4323,  0.1167],\n",
            "         ...,\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         ...,\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505]],\n",
            "\n",
            "        [[ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         ...,\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505]],\n",
            "\n",
            "        [[ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "         ...,\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505],\n",
            "         [-0.4506,  0.2191,  0.4139,  ...,  0.5647, -0.8215, -0.5505]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "PackedSequence(data=tensor([[ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "        [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "        [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "        ...,\n",
            "        [ 0.6022,  0.9307,  0.4561,  ...,  0.8289, -1.3120, -1.0433],\n",
            "        [ 0.4217,  0.7291,  1.1681,  ..., -0.9731,  2.1234,  0.1784],\n",
            "        [ 1.0361,  0.7050,  0.4335,  ..., -0.1493, -0.3842,  0.7111]],\n",
            "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([299, 299, 299, 298, 296, 288, 276, 256, 222, 188, 166, 142, 120,  98,\n",
            "         89,  80,  70,  62,  57,  50,  44,  34,  30,  27,  22,  19,  15,  12,\n",
            "         11,  11,  10,  10,   8,   8,   7,   5,   5,   5,   4,   4,   4,   2,\n",
            "          1,   1]), sorted_indices=tensor([  9, 168,  48, 247, 232, 117,  92, 114, 242,  53,  47, 205, 144, 167,\n",
            "         23, 238,  35, 126,  80,  49,  14, 163, 241, 194, 223, 224, 180, 252,\n",
            "         33,  45, 230, 124, 201, 208, 130, 198,  79,  18, 106,  13,  67,  71,\n",
            "        288, 291, 235, 236,  58, 268, 277, 279,  93,  78, 176, 271, 289, 262,\n",
            "        233,   6,   8, 251, 128, 199,  91, 219, 204,  21, 260, 141, 103, 189,\n",
            "        298,   1, 127, 190, 156, 188,  54,  50,  46,  44, 146, 214, 110,  30,\n",
            "         26,  36,  16,  75,   4, 121, 216, 222, 100, 112, 286, 202,   7,  87,\n",
            "        125,  38, 111,  61,  40, 210, 186, 212, 135, 269, 284, 285, 282, 294,\n",
            "        273,  89,  90, 254,  20, 101,   0,  32, 165, 187, 166,  52, 296, 151,\n",
            "        177, 185,  43, 195,  12, 243,  28,  31, 211,  84, 120, 132,  82,  60,\n",
            "        116,  64, 129, 234, 160, 231, 293, 274, 104, 170, 102, 172, 221, 290,\n",
            "        287, 237,  15, 280, 263, 248, 143, 276, 147,  22, 240, 193, 122, 183,\n",
            "         41, 278, 217, 200, 207, 281, 107, 275,   5, 225, 239,   2,  70,  29,\n",
            "        246, 131, 142, 155,  25,  62, 245, 109,  86,  19, 249, 261, 197, 265,\n",
            "         24, 108, 266,  98, 213, 267, 215, 220, 154, 145,  59, 140, 161,   3,\n",
            "        159, 297, 179, 138, 182,  83,  76,  42, 157, 152, 192, 137, 149, 150,\n",
            "         74, 250, 264, 244, 133,  56,  57, 134,  17, 253, 255, 256, 257,  85,\n",
            "        191, 209, 283, 123,  88, 218,  81, 173,  68, 171, 169,  72, 227, 228,\n",
            "        292,  27, 196, 153, 136, 270,  73, 295, 258, 148,  55, 162, 178, 118,\n",
            "         39, 203,  66,  99, 226, 259,  94, 229,  95,  97,  10, 158, 164,  51,\n",
            "        174,  63, 184, 119, 115, 206,  34,  11,  96, 181,  37, 175, 113, 105,\n",
            "         69, 272,  65,  77, 139]), unsorted_indices=tensor([118,  71, 179, 209,  88, 176,  57,  96,  58,   0, 276, 287, 130,  39,\n",
            "         20, 156,  86, 232,  37, 191, 116,  65, 163,  14, 196, 186,  84, 253,\n",
            "        132, 181,  83, 133, 119,  28, 286,  16,  85, 290,  99, 266, 102, 168,\n",
            "        217, 128,  79,  29,  78,  10,   2,  19,  77, 279, 123,   9,  76, 262,\n",
            "        229, 230,  46, 206, 139, 101, 187, 281, 141, 296, 268,  40, 246, 294,\n",
            "        180,  41, 249, 258, 224,  87, 216, 297,  51,  36,  18, 244, 138, 215,\n",
            "        135, 237, 190,  97, 242, 113, 114,  62,   6,  50, 272, 274, 288, 275,\n",
            "        199, 269,  92, 117, 150,  68, 148, 293,  38, 174, 197, 189,  82, 100,\n",
            "         93, 292,   7, 284, 140,   5, 265, 283, 136,  89, 166, 241,  31,  98,\n",
            "         17,  72,  60, 142,  34, 183, 137, 228, 231, 106, 256, 221, 213, 298,\n",
            "        207,  67, 184, 160,  12, 205,  80, 162, 261, 222, 223, 125, 219, 255,\n",
            "        204, 185,  74, 218, 277, 210, 144, 208, 263,  21, 278, 120, 122,  13,\n",
            "          1, 248, 149, 247, 151, 245, 280, 291,  52, 126, 264, 212,  26, 289,\n",
            "        214, 167, 282, 127, 104, 121,  75,  69,  73, 238, 220, 165,  23, 129,\n",
            "        254, 194,  35,  61, 171,  32,  95, 267,  64,  11, 285, 172,  33, 239,\n",
            "        103, 134, 105, 200,  81, 202,  90, 170, 243,  63, 203, 152,  91,  24,\n",
            "         25, 177, 270, 250, 251, 273,  30, 145,   4,  56, 143,  44,  45, 155,\n",
            "         15, 178, 164,  22,   8, 131, 227, 188, 182,   3, 159, 192, 225,  59,\n",
            "         27, 233, 115, 234, 235, 236, 260, 271,  66, 193,  55, 158, 226, 195,\n",
            "        198, 201,  47, 107, 257,  53, 295, 112, 147, 175, 161,  48, 169,  49,\n",
            "        157, 173, 110, 240, 108, 109,  94, 154,  42,  54, 153,  43, 252, 146,\n",
            "        111, 259, 124, 211,  70]))\n",
            "tensor([[[ 0.3043,  0.1791, -0.4588,  ...,  0.2233,  0.0402,  0.0913],\n",
            "         [ 0.0315, -0.1520,  0.0701,  ...,  0.1185, -0.0697,  0.1475],\n",
            "         [ 0.3384, -0.0528, -0.4308,  ...,  0.3713, -0.0455,  0.1037],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3043,  0.1791, -0.4588,  ...,  0.2233,  0.0402,  0.0913],\n",
            "         [ 0.4667,  0.2941, -0.5486,  ...,  0.3160,  0.0427,  0.1242],\n",
            "         [ 0.5358,  0.3651, -0.5713,  ...,  0.3564,  0.0522,  0.1355],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3043,  0.1791, -0.4588,  ...,  0.2233,  0.0402,  0.0913],\n",
            "         [ 0.1106,  0.2493, -0.5500,  ...,  0.3503,  0.1206,  0.3428],\n",
            "         [ 0.3741, -0.1489, -0.1833,  ...,  0.2151,  0.3053,  0.1597],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.3043,  0.1791, -0.4588,  ...,  0.2233,  0.0402,  0.0913],\n",
            "         [ 0.4667,  0.2941, -0.5486,  ...,  0.3160,  0.0427,  0.1242],\n",
            "         [ 0.5358,  0.3651, -0.5713,  ...,  0.3564,  0.0522,  0.1355],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3043,  0.1791, -0.4588,  ...,  0.2233,  0.0402,  0.0913],\n",
            "         [ 0.4667,  0.2941, -0.5486,  ...,  0.3160,  0.0427,  0.1242],\n",
            "         [ 0.5358,  0.3651, -0.5713,  ...,  0.3564,  0.0522,  0.1355],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3043,  0.1791, -0.4588,  ...,  0.2233,  0.0402,  0.0913],\n",
            "         [ 0.4667,  0.2941, -0.5486,  ...,  0.3160,  0.0427,  0.1242],\n",
            "         [ 0.5358,  0.3651, -0.5713,  ...,  0.3564,  0.0522,  0.1355],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "       grad_fn=<IndexSelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset.df.question_text.to_list()))"
      ],
      "metadata": {
        "id": "jIL7yAJG2-6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4953998-0431-47aa-cc73-2766d3923ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1306122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "LO0QdvaSt41N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Network: LSTM\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, bidirectional=False, dropout=0.0):\n",
        "        super(Network, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocab_size+2, embedding_dim)\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim) # add embedding function\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim, hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True) # initialize the network\n",
        "\n",
        "        # is bidirectional\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # the output layer\n",
        "        # print(\"bidirectional\", bidirectional)\n",
        "        self.linear = nn.Linear(hidden_dim*2, 1)\n",
        "        '''\n",
        "        if (bidirectional):\n",
        "            self.linear = nn.Linear(hidden_dim*2, 1)\n",
        "        else:\n",
        "            self.linear = nn.Linear(hidden_dim, 1)\n",
        "        '''\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, padded, sentence_lengths):\n",
        "        # initialize words to embeddings\n",
        "        # sentence -> word indices -> pad\n",
        "        # -> embedding -> pack -> train -> pad\n",
        "        embedded = self.embedding_layer(padded)\n",
        "        packed_input = pack_padded_sequence(embedded, sentence_lengths.cpu().long(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        # go through the model\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        '''\n",
        "        # get the final hidden state\n",
        "        hidden = torch.torch.randn(1)\n",
        "        if (self.bidirectional):\n",
        "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1]\n",
        "\n",
        "        # dropout\n",
        "        hidden = self.Dropout(hidden)\n",
        "        '''\n",
        "\n",
        "        #output = torch.cat((output[:,-1,:], output[:,-2,:]), dim=1).squeeze(1)\n",
        "\n",
        "        out = self.linear(output[:,-1,:])\n",
        "        return out"
      ],
      "metadata": {
        "id": "pOBWjoQ4uDRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training function\n",
        "def train(train_dataset, test_dataset, network, optimizer, criterion, hyperparameters, n_eval):\n",
        "    avg_acc = [0.]\n",
        "    avg_loss = [0.]\n",
        "\n",
        "    # Get keyword arguments\n",
        "    batch_size, epochs = hyperparameters[\"batch_size\"], hyperparameters[\"epochs\"]\n",
        "\n",
        "    # Initialize dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    network.train() # set the module in training mode\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1} of {epochs}\")\n",
        "\n",
        "        # Loop over each batch in the dataset\n",
        "        for step, (batch_data, batch_label) in enumerate(tqdm(train_loader, desc=f\"Training | Loss: {avg_loss[-1]} | Acc: {avg_acc[-1]}\")):\n",
        "            network.train()\n",
        "\n",
        "            # network() calls forward() in the model\n",
        "            # torch.squeeze() removes all dimensions of size 1\n",
        "\n",
        "            # batch_data: [padded input, sentence lengths]\n",
        "            pred = network(batch_data[0], batch_data[1]).squeeze(1)\n",
        "\n",
        "            # calculate loss for bp\n",
        "            # criterion is the self-chosen loss function\n",
        "            loss = criterion(pred, batch_label)\n",
        "\n",
        "            # calculate accuracy\n",
        "            # binary_acc returns a float number of accuracy\n",
        "            # torch.Tensor.item() change tensor with one number to a python number\n",
        "            acc = binary_acc(pred, batch_label).item()\n",
        "            avg_acc.append(acc)\n",
        "            avg_loss.append(loss.item())\n",
        "\n",
        "            # optimizer from nn.Module\n",
        "            optimizer.zero_grad()\n",
        "            # back propagation\n",
        "            loss.backward()\n",
        "            # optimizer updates the parameters\n",
        "            # have access to parameters when initialized\n",
        "            optimizer.step()\n",
        "\n",
        "            # Periodically evaluate our model + log to Tensorboard\n",
        "            if n_eval != 0 and step != 0 and step % n_eval == 0:\n",
        "                # Log the training accuracy to Tensorboard.\n",
        "                print(\"Train Loss: {}\".format(loss))\n",
        "                out_acc = np.array(avg_acc).mean()\n",
        "                print(\"Train Average Accuracy: {}\".format(out_acc))\n",
        "\n",
        "                # Compute test loss and accuracy.\n",
        "                # Log the results to Tensorboard.\n",
        "                # Don't forget to turn off gradient calculations!\n",
        "                evaluation(network, test_loader, criterion)\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        print()\n",
        "\n",
        "    return avg_acc, avg_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "tknbgtqhwKmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy\n",
        "def binary_acc(preds, y):\n",
        "    # torch.round() rounds to integer as default\n",
        "    # equivalent to having 0.5 as the boundary line\n",
        "    # torch.sigmoid() applies a sigmoid to the input\n",
        "    preds = torch.round(torch.sigmoid(preds))\n",
        "    # torch.eq() returns 1 for same and 0 for different\n",
        "    correct = torch.eq(preds, y).float()\n",
        "    # average over batch\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "# evaluation (no training)\n",
        "def evaluation(network, test_loader, criterion):\n",
        "    avg_acc = []\n",
        "    avg_loss = []\n",
        "\n",
        "    network.eval() # set the model to evaluation mode\n",
        "\n",
        "    # \"with\" is a block of code that releases all changes after\n",
        "    # out of the block\n",
        "    # In this case torch.no_grad() disables back propagation,\n",
        "    # which saves memory, and the setting was changed back to\n",
        "    # normal after the \"with\" block\n",
        "    with torch.no_grad():\n",
        "        for step, (batch_data, batch_label) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
        "            # predict through model\n",
        "            pred = network(batch_data[0], batch_data[1]).squeeze(1)\n",
        "            # calculate loss\n",
        "            loss = criterion(pred, batch_label)\n",
        "            avg_loss.append(loss.item())\n",
        "            # calculate accuracy\n",
        "            acc = binary_acc(pred, batch_label).item()\n",
        "            avg_acc.append(acc)\n",
        "\n",
        "    # calculate mean loss and mean accuracy\n",
        "    out_loss = np.array(avg_loss).mean()\n",
        "    out_acc = np.array(avg_acc).mean()\n",
        "    # output mean loss and mean accuracy\n",
        "    print(\"\\nTrain Loss: {}\".format(out_loss))\n",
        "    print(\"Test average accuracy: {}\".format(out_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "TAVGBlfF-m-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize dataset\n",
        "\n",
        "data_size = 1306122\n",
        "train_proportion = 0.8\n",
        "\n",
        "dataset = StartingDataset(path, device, data_size)\n",
        "data_size = dataset.padded.size(0)\n",
        "print(data_size)\n",
        "train_size = int(train_proportion * data_size)\n",
        "test_size = data_size - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "print(train_dataset, test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqb3yquEGrew",
        "outputId": "404416df-b833-45a7-fb8d-0d3058591724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1306122\n",
            "<torch.utils.data.dataset.Subset object at 0x7ca28efc85e0> <torch.utils.data.dataset.Subset object at 0x7ca28e670910>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.padded.size())\n",
        "print(dataset.sentence_lengths.size())\n",
        "print(dataset.labels.size())\n",
        "print(dataset.padded.device)\n",
        "print(dataset.sentence_lengths.device)\n",
        "print(dataset.labels.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBocYEgEbQsx",
        "outputId": "0710a15b-8ca8-487e-ea03-20bcd9386a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1306122, 128])\n",
            "torch.Size([1306122])\n",
            "torch.Size([1306122])\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "num_layers = 1\n",
        "bidirectional = True\n",
        "dropout = 0.0\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "learning_rate = 1e-2\n",
        "n_eval = train_size // batch_size - 1\n",
        "\n",
        "network = Network(len(dataset.token2idx), embedding_dim, hidden_dim, num_layers, bidirectional, dropout)\n",
        "network.to(device)\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "hyperparameters = {\"batch_size\": batch_size, \"epochs\": epochs}\n"
      ],
      "metadata": {
        "id": "SBktfmK2K3RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_acc, avg_loss = train(train_dataset, test_dataset, network, optimizer, criterion, hyperparameters, n_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTVy2aGtV9Wq",
        "outputId": "748166f6-6aac-4798-b7c7-68a953c4a374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.0 | Acc: 0.0: 100%|█████████▉| 10444/10449 [02:09<00:00, 83.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.22723178565502167\n",
            "Train Average Accuracy: 0.9377949871133533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 24/2613 [00:00<00:10, 239.87it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 49/2613 [00:00<00:10, 241.25it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 74/2613 [00:00<00:10, 244.20it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 100/2613 [00:00<00:10, 249.03it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 125/2613 [00:00<00:10, 248.35it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 150/2613 [00:00<00:10, 238.25it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 174/2613 [00:00<00:10, 234.33it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 199/2613 [00:00<00:10, 237.53it/s]\u001b[A\n",
            "Evaluating:   9%|▊         | 223/2613 [00:00<00:10, 234.97it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 248/2613 [00:01<00:09, 239.20it/s]\u001b[A\n",
            "Evaluating:  10%|█         | 274/2613 [00:01<00:09, 243.23it/s]\u001b[A\n",
            "Evaluating:  11%|█▏        | 299/2613 [00:01<00:09, 244.11it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 324/2613 [00:01<00:09, 245.35it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 349/2613 [00:01<00:09, 244.85it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 374/2613 [00:01<00:09, 245.68it/s]\u001b[A\n",
            "Evaluating:  15%|█▌        | 399/2613 [00:01<00:09, 231.57it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 423/2613 [00:01<00:09, 230.02it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 448/2613 [00:01<00:09, 234.70it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 472/2613 [00:01<00:09, 232.75it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 497/2613 [00:02<00:08, 236.04it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 521/2613 [00:02<00:08, 237.04it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 546/2613 [00:02<00:08, 238.22it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 571/2613 [00:02<00:08, 241.39it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 596/2613 [00:02<00:08, 240.48it/s]\u001b[A\n",
            "Evaluating:  24%|██▍       | 621/2613 [00:02<00:08, 232.36it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 645/2613 [00:02<00:08, 226.24it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 670/2613 [00:02<00:08, 232.25it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 694/2613 [00:02<00:08, 230.64it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 718/2613 [00:03<00:08, 232.92it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 744/2613 [00:03<00:07, 238.09it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 768/2613 [00:03<00:07, 238.41it/s]\u001b[A\n",
            "Evaluating:  30%|███       | 794/2613 [00:03<00:07, 242.32it/s]\u001b[A\n",
            "Evaluating:  31%|███▏      | 819/2613 [00:03<00:07, 243.35it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 844/2613 [00:03<00:07, 244.23it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 869/2613 [00:03<00:07, 229.86it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 893/2613 [00:03<00:07, 229.66it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 918/2613 [00:03<00:07, 234.47it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 942/2613 [00:03<00:07, 233.80it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 967/2613 [00:04<00:06, 237.41it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 992/2613 [00:04<00:06, 239.54it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1018/2613 [00:04<00:06, 243.41it/s]\u001b[A\n",
            "Evaluating:  40%|███▉      | 1043/2613 [00:04<00:06, 243.33it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1068/2613 [00:04<00:06, 244.26it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1093/2613 [00:04<00:06, 238.88it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1117/2613 [00:04<00:06, 226.83it/s]\u001b[A\n",
            "Evaluating:  44%|████▎     | 1140/2613 [00:04<00:06, 220.47it/s]\u001b[A\n",
            "Evaluating:  45%|████▍     | 1165/2613 [00:04<00:06, 227.37it/s]\u001b[A\n",
            "Evaluating:  45%|████▌     | 1188/2613 [00:05<00:06, 226.13it/s]\u001b[A\n",
            "Evaluating:  46%|████▋     | 1213/2613 [00:05<00:06, 231.86it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1238/2613 [00:05<00:05, 236.42it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1262/2613 [00:05<00:05, 236.94it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1286/2613 [00:05<00:05, 237.43it/s]\u001b[A\n",
            "Evaluating:  50%|█████     | 1311/2613 [00:05<00:05, 240.96it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1336/2613 [00:05<00:05, 229.10it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1360/2613 [00:05<00:05, 230.15it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1385/2613 [00:05<00:05, 234.68it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1409/2613 [00:05<00:05, 235.58it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1433/2613 [00:06<00:05, 234.78it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1458/2613 [00:06<00:04, 236.41it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1483/2613 [00:06<00:04, 239.60it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1508/2613 [00:06<00:04, 240.58it/s]\u001b[A\n",
            "Evaluating:  59%|█████▊    | 1534/2613 [00:06<00:04, 243.82it/s]\u001b[A\n",
            "Evaluating:  60%|█████▉    | 1559/2613 [00:06<00:04, 238.69it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1583/2613 [00:06<00:04, 229.03it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1607/2613 [00:06<00:04, 230.65it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1632/2613 [00:06<00:04, 235.21it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1658/2613 [00:07<00:03, 239.93it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1683/2613 [00:07<00:03, 235.06it/s]\u001b[A\n",
            "Evaluating:  65%|██████▌   | 1708/2613 [00:07<00:03, 237.38it/s]\u001b[A\n",
            "Evaluating:  66%|██████▋   | 1733/2613 [00:07<00:03, 239.83it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1758/2613 [00:07<00:03, 241.84it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1783/2613 [00:07<00:03, 242.68it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1808/2613 [00:07<00:03, 231.90it/s]\u001b[A\n",
            "Evaluating:  70%|███████   | 1832/2613 [00:07<00:03, 230.88it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1856/2613 [00:07<00:03, 232.55it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1880/2613 [00:07<00:03, 232.96it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1904/2613 [00:08<00:03, 234.04it/s]\u001b[A\n",
            "Evaluating:  74%|███████▍  | 1928/2613 [00:08<00:02, 233.67it/s]\u001b[A\n",
            "Evaluating:  75%|███████▍  | 1952/2613 [00:08<00:02, 233.44it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1976/2613 [00:08<00:02, 233.09it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2001/2613 [00:08<00:02, 236.30it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2026/2613 [00:08<00:02, 239.50it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2050/2613 [00:08<00:02, 229.49it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2076/2613 [00:08<00:02, 235.98it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2101/2613 [00:08<00:02, 238.07it/s]\u001b[A\n",
            "Evaluating:  81%|████████▏ | 2126/2613 [00:08<00:02, 240.63it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2151/2613 [00:09<00:01, 238.88it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2175/2613 [00:09<00:01, 226.56it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2198/2613 [00:09<00:02, 205.99it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2219/2613 [00:09<00:02, 196.16it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2239/2613 [00:09<00:02, 175.15it/s]\u001b[A\n",
            "Evaluating:  86%|████████▋ | 2258/2613 [00:09<00:02, 167.57it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2276/2613 [00:09<00:02, 167.05it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2294/2613 [00:09<00:01, 168.11it/s]\u001b[A\n",
            "Evaluating:  89%|████████▊ | 2313/2613 [00:10<00:01, 171.85it/s]\u001b[A\n",
            "Evaluating:  89%|████████▉ | 2331/2613 [00:10<00:01, 171.45it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2349/2613 [00:10<00:01, 173.81it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2368/2613 [00:10<00:01, 177.51it/s]\u001b[A\n",
            "Evaluating:  91%|█████████▏| 2386/2613 [00:10<00:01, 175.58it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2404/2613 [00:10<00:01, 162.36it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2421/2613 [00:10<00:01, 160.23it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2438/2613 [00:10<00:01, 159.92it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2455/2613 [00:10<00:00, 160.53it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▍| 2472/2613 [00:11<00:00, 158.20it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▌| 2489/2613 [00:11<00:00, 161.45it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2506/2613 [00:11<00:00, 158.29it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2522/2613 [00:11<00:00, 158.46it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2540/2613 [00:11<00:00, 162.05it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2557/2613 [00:11<00:00, 159.09it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2573/2613 [00:11<00:00, 150.24it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▉| 2590/2613 [00:11<00:00, 154.99it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:11<00:00, 218.19it/s]\n",
            "Training | Loss: 0.0 | Acc: 0.0: 100%|██████████| 10449/10449 [02:22<00:00, 73.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23282432332321276\n",
            "Test average accuracy: 0.9374779775322419\n",
            "\n",
            "Epoch 2 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.17847782373428345 | Acc: 0.9587628841400146: 100%|█████████▉| 10439/10449 [02:09<00:00, 88.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2527020275592804\n",
            "Train Average Accuracy: 0.9379169491196959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 23/2613 [00:00<00:11, 229.45it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 46/2613 [00:00<00:11, 217.14it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 68/2613 [00:00<00:13, 187.87it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 88/2613 [00:00<00:13, 181.45it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 107/2613 [00:00<00:13, 180.61it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 126/2613 [00:00<00:13, 179.48it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 145/2613 [00:00<00:13, 176.88it/s]\u001b[A\n",
            "Evaluating:   6%|▋         | 164/2613 [00:00<00:13, 179.70it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 183/2613 [00:00<00:13, 182.37it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 202/2613 [00:01<00:13, 183.09it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 221/2613 [00:01<00:13, 179.80it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 240/2613 [00:01<00:13, 176.47it/s]\u001b[A\n",
            "Evaluating:  10%|▉         | 259/2613 [00:01<00:13, 180.09it/s]\u001b[A\n",
            "Evaluating:  11%|█         | 278/2613 [00:01<00:12, 179.79it/s]\u001b[A\n",
            "Evaluating:  11%|█▏        | 297/2613 [00:01<00:13, 174.61it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 315/2613 [00:01<00:13, 169.93it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 333/2613 [00:01<00:13, 168.38it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 350/2613 [00:01<00:13, 166.99it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 367/2613 [00:02<00:13, 165.25it/s]\u001b[A\n",
            "Evaluating:  15%|█▍        | 384/2613 [00:02<00:13, 164.15it/s]\u001b[A\n",
            "Evaluating:  15%|█▌        | 401/2613 [00:02<00:13, 161.62it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 418/2613 [00:02<00:14, 154.31it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 434/2613 [00:02<00:14, 154.22it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 451/2613 [00:02<00:13, 157.24it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 468/2613 [00:02<00:13, 159.32it/s]\u001b[A\n",
            "Evaluating:  19%|█▊        | 484/2613 [00:02<00:13, 156.10it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 501/2613 [00:02<00:13, 159.29it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 518/2613 [00:03<00:13, 159.83it/s]\u001b[A\n",
            "Evaluating:  20%|██        | 535/2613 [00:03<00:12, 160.74it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 552/2613 [00:03<00:14, 143.83it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 567/2613 [00:03<00:14, 143.60it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 584/2613 [00:03<00:13, 149.83it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 608/2613 [00:03<00:11, 174.58it/s]\u001b[A\n",
            "Evaluating:  24%|██▍       | 633/2613 [00:03<00:10, 193.98it/s]\u001b[A\n",
            "Evaluating:  25%|██▌       | 657/2613 [00:03<00:09, 206.65it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 680/2613 [00:03<00:09, 211.35it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 704/2613 [00:04<00:08, 218.76it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 730/2613 [00:04<00:08, 228.16it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 753/2613 [00:04<00:08, 227.36it/s]\u001b[A\n",
            "Evaluating:  30%|██▉       | 776/2613 [00:04<00:08, 216.41it/s]\u001b[A\n",
            "Evaluating:  31%|███       | 801/2613 [00:04<00:08, 224.10it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 824/2613 [00:04<00:07, 225.64it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 849/2613 [00:04<00:07, 232.42it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 874/2613 [00:04<00:07, 236.15it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 899/2613 [00:04<00:07, 239.23it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 923/2613 [00:04<00:07, 234.18it/s]\u001b[A\n",
            "Evaluating:  36%|███▋      | 949/2613 [00:05<00:06, 239.34it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 973/2613 [00:05<00:06, 238.76it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 997/2613 [00:05<00:06, 234.26it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1021/2613 [00:05<00:07, 226.51it/s]\u001b[A\n",
            "Evaluating:  40%|████      | 1046/2613 [00:05<00:06, 231.41it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1070/2613 [00:05<00:06, 232.75it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1095/2613 [00:05<00:06, 237.70it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1121/2613 [00:05<00:06, 241.30it/s]\u001b[A\n",
            "Evaluating:  44%|████▍     | 1146/2613 [00:05<00:06, 236.26it/s]\u001b[A\n",
            "Evaluating:  45%|████▍     | 1171/2613 [00:05<00:06, 239.31it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1196/2613 [00:06<00:05, 239.76it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1221/2613 [00:06<00:05, 242.36it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1246/2613 [00:06<00:06, 227.40it/s]\u001b[A\n",
            "Evaluating:  49%|████▊     | 1271/2613 [00:06<00:05, 232.00it/s]\u001b[A\n",
            "Evaluating:  50%|████▉     | 1295/2613 [00:06<00:05, 234.06it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1320/2613 [00:06<00:05, 236.57it/s]\u001b[A\n",
            "Evaluating:  51%|█████▏    | 1345/2613 [00:06<00:05, 239.76it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1371/2613 [00:06<00:05, 244.14it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1396/2613 [00:06<00:05, 238.04it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1422/2613 [00:07<00:04, 242.07it/s]\u001b[A\n",
            "Evaluating:  55%|█████▌    | 1447/2613 [00:07<00:04, 243.14it/s]\u001b[A\n",
            "Evaluating:  56%|█████▋    | 1472/2613 [00:07<00:04, 232.96it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1496/2613 [00:07<00:04, 225.64it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1521/2613 [00:07<00:04, 231.59it/s]\u001b[A\n",
            "Evaluating:  59%|█████▉    | 1546/2613 [00:07<00:04, 234.88it/s]\u001b[A\n",
            "Evaluating:  60%|██████    | 1571/2613 [00:07<00:04, 238.82it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1596/2613 [00:07<00:04, 240.51it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1621/2613 [00:07<00:04, 242.17it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1646/2613 [00:07<00:04, 237.85it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1671/2613 [00:08<00:03, 240.10it/s]\u001b[A\n",
            "Evaluating:  65%|██████▍   | 1696/2613 [00:08<00:03, 239.85it/s]\u001b[A\n",
            "Evaluating:  66%|██████▌   | 1721/2613 [00:08<00:03, 227.86it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1746/2613 [00:08<00:03, 231.83it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1770/2613 [00:08<00:03, 233.46it/s]\u001b[A\n",
            "Evaluating:  69%|██████▊   | 1795/2613 [00:08<00:03, 237.48it/s]\u001b[A\n",
            "Evaluating:  70%|██████▉   | 1820/2613 [00:08<00:03, 238.29it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1845/2613 [00:08<00:03, 239.59it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1870/2613 [00:08<00:03, 242.32it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1895/2613 [00:09<00:03, 234.38it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1920/2613 [00:09<00:02, 238.73it/s]\u001b[A\n",
            "Evaluating:  74%|███████▍  | 1944/2613 [00:09<00:02, 232.58it/s]\u001b[A\n",
            "Evaluating:  75%|███████▌  | 1968/2613 [00:09<00:02, 223.85it/s]\u001b[A\n",
            "Evaluating:  76%|███████▋  | 1993/2613 [00:09<00:02, 229.24it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2017/2613 [00:09<00:02, 230.42it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2042/2613 [00:09<00:02, 235.73it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2067/2613 [00:09<00:02, 239.79it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2092/2613 [00:09<00:02, 239.17it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2116/2613 [00:09<00:02, 234.95it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2142/2613 [00:10<00:01, 239.73it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2167/2613 [00:10<00:01, 238.70it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2191/2613 [00:10<00:01, 227.64it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2216/2613 [00:10<00:01, 233.23it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2240/2613 [00:10<00:01, 234.79it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2266/2613 [00:10<00:01, 239.49it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2292/2613 [00:10<00:01, 243.15it/s]\u001b[A\n",
            "Evaluating:  89%|████████▊ | 2317/2613 [00:10<00:01, 244.23it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2342/2613 [00:10<00:01, 243.28it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2367/2613 [00:11<00:01, 239.15it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2392/2613 [00:11<00:00, 239.90it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2417/2613 [00:11<00:00, 232.73it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2441/2613 [00:11<00:00, 230.51it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2466/2613 [00:11<00:00, 234.13it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▌| 2491/2613 [00:11<00:00, 236.72it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2515/2613 [00:11<00:00, 237.52it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2540/2613 [00:11<00:00, 240.79it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2565/2613 [00:11<00:00, 242.28it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:12<00:00, 215.97it/s]\n",
            "Training | Loss: 0.17847782373428345 | Acc: 0.9587628841400146: 100%|██████████| 10449/10449 [02:21<00:00, 73.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23291846124937923\n",
            "Test average accuracy: 0.9374932853417336\n",
            "\n",
            "Epoch 3 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.11971396952867508 | Acc: 0.9793814420700073: 100%|█████████▉| 10438/10449 [02:10<00:00, 87.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2839634120464325\n",
            "Train Average Accuracy: 0.9379547518532816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 23/2613 [00:00<00:11, 229.26it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 49/2613 [00:00<00:10, 242.18it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 74/2613 [00:00<00:10, 242.59it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 99/2613 [00:00<00:10, 235.37it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 123/2613 [00:00<00:10, 234.69it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 147/2613 [00:00<00:10, 225.33it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 172/2613 [00:00<00:10, 232.51it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 197/2613 [00:00<00:10, 236.63it/s]\u001b[A\n",
            "Evaluating:   9%|▊         | 223/2613 [00:00<00:09, 240.53it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 248/2613 [00:01<00:09, 243.10it/s]\u001b[A\n",
            "Evaluating:  10%|█         | 273/2613 [00:01<00:09, 242.22it/s]\u001b[A\n",
            "Evaluating:  11%|█▏        | 298/2613 [00:01<00:09, 243.85it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 323/2613 [00:01<00:09, 244.47it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 348/2613 [00:01<00:09, 238.92it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 372/2613 [00:01<00:09, 230.28it/s]\u001b[A\n",
            "Evaluating:  15%|█▌        | 396/2613 [00:01<00:09, 227.52it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 421/2613 [00:01<00:09, 231.98it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 445/2613 [00:01<00:09, 233.63it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 470/2613 [00:01<00:09, 235.99it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 494/2613 [00:02<00:08, 235.90it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 518/2613 [00:02<00:08, 235.63it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 542/2613 [00:02<00:08, 235.06it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 566/2613 [00:02<00:09, 227.13it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 590/2613 [00:02<00:08, 229.36it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 613/2613 [00:02<00:09, 216.86it/s]\u001b[A\n",
            "Evaluating:  24%|██▍       | 635/2613 [00:02<00:09, 217.71it/s]\u001b[A\n",
            "Evaluating:  25%|██▌       | 659/2613 [00:02<00:08, 222.79it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 683/2613 [00:02<00:08, 226.75it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 708/2613 [00:03<00:08, 232.81it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 732/2613 [00:03<00:08, 234.31it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 757/2613 [00:03<00:07, 237.93it/s]\u001b[A\n",
            "Evaluating:  30%|██▉       | 782/2613 [00:03<00:07, 241.00it/s]\u001b[A\n",
            "Evaluating:  31%|███       | 807/2613 [00:03<00:07, 234.54it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 831/2613 [00:03<00:07, 225.32it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 854/2613 [00:03<00:07, 220.13it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 878/2613 [00:03<00:07, 224.97it/s]\u001b[A\n",
            "Evaluating:  35%|███▍      | 903/2613 [00:03<00:07, 230.56it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 927/2613 [00:04<00:08, 210.29it/s]\u001b[A\n",
            "Evaluating:  36%|███▋      | 949/2613 [00:04<00:08, 199.45it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 970/2613 [00:04<00:08, 192.61it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 990/2613 [00:04<00:08, 189.71it/s]\u001b[A\n",
            "Evaluating:  39%|███▊      | 1010/2613 [00:04<00:08, 183.01it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1029/2613 [00:04<00:09, 165.85it/s]\u001b[A\n",
            "Evaluating:  40%|████      | 1047/2613 [00:04<00:09, 169.12it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1066/2613 [00:04<00:08, 173.93it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1085/2613 [00:04<00:08, 177.36it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1104/2613 [00:05<00:08, 180.37it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1123/2613 [00:05<00:08, 181.89it/s]\u001b[A\n",
            "Evaluating:  44%|████▎     | 1142/2613 [00:05<00:08, 178.67it/s]\u001b[A\n",
            "Evaluating:  44%|████▍     | 1160/2613 [00:05<00:08, 170.47it/s]\u001b[A\n",
            "Evaluating:  45%|████▌     | 1178/2613 [00:05<00:08, 170.85it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1196/2613 [00:05<00:09, 153.29it/s]\u001b[A\n",
            "Evaluating:  46%|████▋     | 1212/2613 [00:05<00:09, 148.95it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1228/2613 [00:05<00:09, 151.19it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1245/2613 [00:05<00:08, 155.58it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1263/2613 [00:06<00:08, 160.15it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1280/2613 [00:06<00:08, 162.24it/s]\u001b[A\n",
            "Evaluating:  50%|████▉     | 1297/2613 [00:06<00:08, 164.02it/s]\u001b[A\n",
            "Evaluating:  50%|█████     | 1315/2613 [00:06<00:07, 166.18it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1333/2613 [00:06<00:07, 168.31it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1350/2613 [00:06<00:08, 154.33it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1366/2613 [00:06<00:08, 150.10it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1383/2613 [00:06<00:08, 153.42it/s]\u001b[A\n",
            "Evaluating:  54%|█████▎    | 1401/2613 [00:06<00:07, 158.39it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1418/2613 [00:07<00:07, 160.06it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1435/2613 [00:07<00:07, 159.89it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1452/2613 [00:07<00:07, 161.03it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1469/2613 [00:07<00:07, 162.35it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1486/2613 [00:07<00:06, 164.47it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1508/2613 [00:07<00:06, 180.58it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1527/2613 [00:07<00:06, 178.46it/s]\u001b[A\n",
            "Evaluating:  59%|█████▉    | 1550/2613 [00:07<00:05, 192.68it/s]\u001b[A\n",
            "Evaluating:  60%|██████    | 1573/2613 [00:07<00:05, 201.20it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1597/2613 [00:07<00:04, 211.10it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1621/2613 [00:08<00:04, 218.84it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1645/2613 [00:08<00:04, 223.92it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1670/2613 [00:08<00:04, 230.71it/s]\u001b[A\n",
            "Evaluating:  65%|██████▍   | 1694/2613 [00:08<00:03, 232.79it/s]\u001b[A\n",
            "Evaluating:  66%|██████▌   | 1718/2613 [00:08<00:03, 234.50it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1742/2613 [00:08<00:03, 228.84it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1765/2613 [00:08<00:03, 221.40it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1789/2613 [00:08<00:03, 225.52it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1814/2613 [00:08<00:03, 232.30it/s]\u001b[A\n",
            "Evaluating:  70%|███████   | 1838/2613 [00:08<00:03, 232.35it/s]\u001b[A\n",
            "Evaluating:  71%|███████▏  | 1863/2613 [00:09<00:03, 235.62it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1888/2613 [00:09<00:03, 237.56it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1912/2613 [00:09<00:02, 238.16it/s]\u001b[A\n",
            "Evaluating:  74%|███████▍  | 1936/2613 [00:09<00:02, 238.66it/s]\u001b[A\n",
            "Evaluating:  75%|███████▌  | 1960/2613 [00:09<00:02, 238.83it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1984/2613 [00:09<00:02, 225.29it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2007/2613 [00:09<00:02, 223.30it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2031/2613 [00:09<00:02, 227.83it/s]\u001b[A\n",
            "Evaluating:  79%|███████▊  | 2056/2613 [00:09<00:02, 233.13it/s]\u001b[A\n",
            "Evaluating:  80%|███████▉  | 2080/2613 [00:10<00:02, 234.44it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2105/2613 [00:10<00:02, 237.25it/s]\u001b[A\n",
            "Evaluating:  81%|████████▏ | 2129/2613 [00:10<00:02, 237.83it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2155/2613 [00:10<00:01, 241.71it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2180/2613 [00:10<00:01, 240.94it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2205/2613 [00:10<00:01, 235.62it/s]\u001b[A\n",
            "Evaluating:  85%|████████▌ | 2229/2613 [00:10<00:01, 226.46it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2252/2613 [00:10<00:01, 222.18it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2277/2613 [00:10<00:01, 228.14it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2302/2613 [00:10<00:01, 232.63it/s]\u001b[A\n",
            "Evaluating:  89%|████████▉ | 2327/2613 [00:11<00:01, 235.44it/s]\u001b[A\n",
            "Evaluating:  90%|█████████ | 2352/2613 [00:11<00:01, 238.16it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2377/2613 [00:11<00:00, 240.68it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2402/2613 [00:11<00:00, 241.69it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2427/2613 [00:11<00:00, 243.80it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2452/2613 [00:11<00:00, 229.49it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▍| 2476/2613 [00:11<00:00, 230.41it/s]\u001b[A\n",
            "Training | Loss: 0.11971396952867508 | Acc: 0.9793814420700073: 100%|█████████▉| 10438/10449 [02:22<00:00, 87.34it/s]\n",
            "Evaluating:  97%|█████████▋| 2524/2613 [00:11<00:00, 230.65it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2549/2613 [00:12<00:00, 233.88it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▊| 2574/2613 [00:12<00:00, 236.62it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:12<00:00, 212.44it/s]\n",
            "Training | Loss: 0.11971396952867508 | Acc: 0.9793814420700073: 100%|██████████| 10449/10449 [02:22<00:00, 73.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23286140041473585\n",
            "Test average accuracy: 0.937122063964343\n",
            "\n",
            "Epoch 4 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.29139426350593567 | Acc: 0.9175257682800293: 100%|█████████▉| 10442/10449 [02:10<00:00, 79.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.28162887692451477\n",
            "Train Average Accuracy: 0.93797001983667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 23/2613 [00:00<00:11, 228.56it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 48/2613 [00:00<00:10, 238.35it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 72/2613 [00:00<00:10, 238.24it/s]\u001b[A\n",
            "Evaluating:   4%|▎         | 96/2613 [00:00<00:10, 238.90it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 121/2613 [00:00<00:10, 239.57it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 145/2613 [00:00<00:10, 239.11it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 170/2613 [00:00<00:10, 241.14it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 195/2613 [00:00<00:10, 234.99it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 219/2613 [00:00<00:10, 227.31it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 242/2613 [00:01<00:10, 223.38it/s]\u001b[A\n",
            "Evaluating:  10%|█         | 267/2613 [00:01<00:10, 229.33it/s]\u001b[A\n",
            "Evaluating:  11%|█         | 292/2613 [00:01<00:09, 235.24it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 316/2613 [00:01<00:09, 235.53it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 340/2613 [00:01<00:09, 235.23it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 364/2613 [00:01<00:09, 236.31it/s]\u001b[A\n",
            "Evaluating:  15%|█▍        | 389/2613 [00:01<00:09, 239.97it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 414/2613 [00:01<00:09, 240.65it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 439/2613 [00:01<00:09, 232.96it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 463/2613 [00:01<00:09, 222.34it/s]\u001b[A\n",
            "Evaluating:  19%|█▊        | 488/2613 [00:02<00:09, 227.68it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 514/2613 [00:02<00:08, 235.55it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 540/2613 [00:02<00:08, 240.33it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 566/2613 [00:02<00:08, 244.01it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 591/2613 [00:02<00:08, 245.54it/s]\u001b[A\n",
            "Evaluating:  24%|██▎       | 617/2613 [00:02<00:08, 247.31it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 642/2613 [00:02<00:07, 247.95it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 667/2613 [00:02<00:07, 245.53it/s]\u001b[A\n",
            "Evaluating:  26%|██▋       | 692/2613 [00:02<00:08, 229.94it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 716/2613 [00:03<00:08, 226.12it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 741/2613 [00:03<00:08, 230.52it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 766/2613 [00:03<00:07, 233.83it/s]\u001b[A\n",
            "Evaluating:  30%|███       | 792/2613 [00:03<00:07, 239.03it/s]\u001b[A\n",
            "Evaluating:  31%|███       | 816/2613 [00:03<00:07, 238.54it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 840/2613 [00:03<00:07, 238.19it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 865/2613 [00:03<00:07, 240.41it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 890/2613 [00:03<00:07, 240.13it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 915/2613 [00:03<00:07, 227.02it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 938/2613 [00:03<00:07, 226.20it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 961/2613 [00:04<00:07, 225.60it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 987/2613 [00:04<00:06, 233.52it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1013/2613 [00:04<00:06, 240.61it/s]\u001b[A\n",
            "Evaluating:  40%|███▉      | 1038/2613 [00:04<00:06, 243.19it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1064/2613 [00:04<00:06, 245.72it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1089/2613 [00:04<00:06, 244.34it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1115/2613 [00:04<00:06, 246.47it/s]\u001b[A\n",
            "Evaluating:  44%|████▎     | 1140/2613 [00:04<00:06, 243.04it/s]\u001b[A\n",
            "Evaluating:  45%|████▍     | 1165/2613 [00:04<00:06, 230.94it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1189/2613 [00:05<00:06, 226.60it/s]\u001b[A\n",
            "Evaluating:  46%|████▋     | 1212/2613 [00:05<00:06, 225.67it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1238/2613 [00:05<00:05, 235.04it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1263/2613 [00:05<00:05, 239.18it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1289/2613 [00:05<00:05, 241.87it/s]\u001b[A\n",
            "Evaluating:  50%|█████     | 1314/2613 [00:05<00:05, 242.57it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1339/2613 [00:05<00:05, 241.86it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1364/2613 [00:05<00:05, 243.62it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1389/2613 [00:05<00:05, 231.73it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1413/2613 [00:05<00:05, 233.30it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1437/2613 [00:06<00:05, 231.76it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1461/2613 [00:06<00:04, 232.87it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1485/2613 [00:06<00:04, 234.60it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1510/2613 [00:06<00:04, 239.05it/s]\u001b[A\n",
            "Evaluating:  59%|█████▊    | 1535/2613 [00:06<00:04, 239.91it/s]\u001b[A\n",
            "Evaluating:  60%|█████▉    | 1560/2613 [00:06<00:04, 241.02it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1585/2613 [00:06<00:04, 242.23it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1610/2613 [00:06<00:04, 211.58it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1632/2613 [00:06<00:05, 194.84it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1653/2613 [00:07<00:05, 183.69it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1672/2613 [00:07<00:05, 182.68it/s]\u001b[A\n",
            "Evaluating:  65%|██████▍   | 1691/2613 [00:07<00:05, 182.02it/s]\u001b[A\n",
            "Evaluating:  65%|██████▌   | 1710/2613 [00:07<00:04, 181.84it/s]\u001b[A\n",
            "Evaluating:  66%|██████▌   | 1729/2613 [00:07<00:04, 180.36it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1748/2613 [00:07<00:04, 176.89it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1766/2613 [00:07<00:04, 175.66it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1784/2613 [00:07<00:04, 172.41it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1803/2613 [00:07<00:04, 174.75it/s]\u001b[A\n",
            "Evaluating:  70%|██████▉   | 1822/2613 [00:08<00:04, 178.36it/s]\u001b[A\n",
            "Evaluating:  70%|███████   | 1840/2613 [00:08<00:04, 171.15it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1858/2613 [00:08<00:04, 166.58it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1876/2613 [00:08<00:04, 168.31it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1893/2613 [00:08<00:04, 165.57it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1910/2613 [00:08<00:04, 163.47it/s]\u001b[A\n",
            "Evaluating:  74%|███████▎  | 1927/2613 [00:08<00:04, 162.44it/s]\u001b[A\n",
            "Evaluating:  74%|███████▍  | 1944/2613 [00:08<00:04, 159.67it/s]\u001b[A\n",
            "Evaluating:  75%|███████▌  | 1960/2613 [00:08<00:04, 154.37it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1976/2613 [00:09<00:04, 154.00it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1992/2613 [00:09<00:04, 154.62it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2008/2613 [00:09<00:03, 153.54it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2025/2613 [00:09<00:03, 156.08it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2042/2613 [00:09<00:03, 159.46it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2059/2613 [00:09<00:03, 162.41it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2076/2613 [00:09<00:03, 162.07it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2093/2613 [00:09<00:03, 159.93it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2110/2613 [00:09<00:03, 156.05it/s]\u001b[A\n",
            "Evaluating:  81%|████████▏ | 2126/2613 [00:10<00:03, 155.32it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2142/2613 [00:10<00:03, 156.49it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2165/2613 [00:10<00:02, 175.26it/s]\u001b[A\n",
            "Evaluating:  84%|████████▎ | 2186/2613 [00:10<00:02, 184.99it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2210/2613 [00:10<00:02, 200.76it/s]\u001b[A\n",
            "Evaluating:  85%|████████▌ | 2233/2613 [00:10<00:01, 208.25it/s]\u001b[A\n",
            "Evaluating:  86%|████████▋ | 2257/2613 [00:10<00:01, 216.31it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2280/2613 [00:10<00:01, 220.07it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2303/2613 [00:10<00:01, 220.75it/s]\u001b[A\n",
            "Evaluating:  89%|████████▉ | 2326/2613 [00:10<00:01, 215.91it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2350/2613 [00:11<00:01, 220.64it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2375/2613 [00:11<00:01, 227.78it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2400/2613 [00:11<00:00, 232.44it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2424/2613 [00:11<00:00, 226.92it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▎| 2447/2613 [00:11<00:00, 227.30it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▍| 2472/2613 [00:11<00:00, 232.51it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2496/2613 [00:11<00:00, 230.62it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▋| 2521/2613 [00:11<00:00, 233.99it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2545/2613 [00:11<00:00, 225.85it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2568/2613 [00:11<00:00, 226.34it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:12<00:00, 214.52it/s]\n",
            "Training | Loss: 0.29139426350593567 | Acc: 0.9175257682800293: 100%|██████████| 10449/10449 [02:22<00:00, 73.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23257163442725476\n",
            "Test average accuracy: 0.9373746479419836\n",
            "\n",
            "Epoch 5 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.23171235620975494 | Acc: 0.938144326210022: 100%|█████████▉| 10439/10449 [02:10<00:00, 78.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.22892054915428162\n",
            "Train Average Accuracy: 0.9379839776968177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 24/2613 [00:00<00:11, 234.15it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 49/2613 [00:00<00:10, 238.89it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 73/2613 [00:00<00:10, 232.54it/s]\u001b[A\n",
            "Evaluating:   4%|▎         | 97/2613 [00:00<00:10, 235.25it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 123/2613 [00:00<00:10, 241.59it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 148/2613 [00:00<00:10, 240.71it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 174/2613 [00:00<00:10, 242.70it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 199/2613 [00:00<00:09, 243.34it/s]\u001b[A\n",
            "Evaluating:   9%|▊         | 224/2613 [00:00<00:10, 235.65it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 248/2613 [00:01<00:10, 226.22it/s]\u001b[A\n",
            "Evaluating:  10%|█         | 272/2613 [00:01<00:10, 228.19it/s]\u001b[A\n",
            "Evaluating:  11%|█▏        | 297/2613 [00:01<00:09, 233.19it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 321/2613 [00:01<00:09, 230.64it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 345/2613 [00:01<00:09, 232.76it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 371/2613 [00:01<00:09, 238.97it/s]\u001b[A\n",
            "Evaluating:  15%|█▌        | 396/2613 [00:01<00:09, 241.11it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 422/2613 [00:01<00:08, 243.93it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 447/2613 [00:01<00:08, 242.73it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 472/2613 [00:02<00:09, 230.12it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 497/2613 [00:02<00:09, 234.85it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 521/2613 [00:02<00:08, 235.73it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 546/2613 [00:02<00:08, 238.11it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 570/2613 [00:02<00:08, 233.03it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 595/2613 [00:02<00:08, 237.34it/s]\u001b[A\n",
            "Evaluating:  24%|██▎       | 620/2613 [00:02<00:08, 239.02it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 646/2613 [00:02<00:08, 242.57it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 671/2613 [00:02<00:07, 243.80it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 696/2613 [00:02<00:08, 236.90it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 720/2613 [00:03<00:08, 226.68it/s]\u001b[A\n",
            "Evaluating:  29%|██▊       | 745/2613 [00:03<00:08, 231.44it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 770/2613 [00:03<00:07, 236.20it/s]\u001b[A\n",
            "Evaluating:  30%|███       | 794/2613 [00:03<00:07, 235.54it/s]\u001b[A\n",
            "Evaluating:  31%|███▏      | 818/2613 [00:03<00:07, 235.60it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 843/2613 [00:03<00:07, 237.75it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 869/2613 [00:03<00:07, 241.61it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 894/2613 [00:03<00:07, 240.99it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 919/2613 [00:03<00:06, 242.46it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 944/2613 [00:04<00:07, 227.72it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 967/2613 [00:04<00:07, 223.83it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 991/2613 [00:04<00:07, 227.60it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1016/2613 [00:04<00:06, 233.25it/s]\u001b[A\n",
            "Evaluating:  40%|███▉      | 1040/2613 [00:04<00:06, 228.04it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1065/2613 [00:04<00:06, 232.66it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1090/2613 [00:04<00:06, 236.59it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1114/2613 [00:04<00:06, 236.75it/s]\u001b[A\n",
            "Evaluating:  44%|████▎     | 1140/2613 [00:04<00:06, 241.64it/s]\u001b[A\n",
            "Evaluating:  45%|████▍     | 1165/2613 [00:04<00:06, 234.15it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1189/2613 [00:05<00:06, 228.13it/s]\u001b[A\n",
            "Evaluating:  46%|████▋     | 1213/2613 [00:05<00:06, 230.17it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1238/2613 [00:05<00:05, 232.66it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1263/2613 [00:05<00:05, 235.68it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1287/2613 [00:05<00:05, 229.66it/s]\u001b[A\n",
            "Evaluating:  50%|█████     | 1311/2613 [00:05<00:05, 231.83it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1336/2613 [00:05<00:05, 236.91it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1361/2613 [00:05<00:05, 238.08it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1386/2613 [00:05<00:05, 240.49it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1411/2613 [00:06<00:05, 222.54it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1436/2613 [00:06<00:05, 228.28it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1461/2613 [00:06<00:04, 232.13it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1486/2613 [00:06<00:04, 235.64it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1512/2613 [00:06<00:04, 240.27it/s]\u001b[A\n",
            "Evaluating:  59%|█████▉    | 1537/2613 [00:06<00:04, 233.01it/s]\u001b[A\n",
            "Evaluating:  60%|█████▉    | 1562/2613 [00:06<00:04, 236.20it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1587/2613 [00:06<00:04, 239.51it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1612/2613 [00:06<00:04, 239.26it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1636/2613 [00:06<00:04, 229.06it/s]\u001b[A\n",
            "Evaluating:  64%|██████▎   | 1660/2613 [00:07<00:04, 225.06it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1685/2613 [00:07<00:04, 229.97it/s]\u001b[A\n",
            "Evaluating:  65%|██████▌   | 1710/2613 [00:07<00:03, 235.54it/s]\u001b[A\n",
            "Evaluating:  66%|██████▋   | 1734/2613 [00:07<00:03, 236.33it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1758/2613 [00:07<00:03, 232.16it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1782/2613 [00:07<00:03, 218.67it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1805/2613 [00:07<00:03, 214.58it/s]\u001b[A\n",
            "Evaluating:  70%|██████▉   | 1829/2613 [00:07<00:03, 221.48it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1853/2613 [00:07<00:03, 224.26it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1876/2613 [00:08<00:03, 217.61it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1901/2613 [00:08<00:03, 225.15it/s]\u001b[A\n",
            "Evaluating:  74%|███████▎  | 1925/2613 [00:08<00:03, 228.74it/s]\u001b[A\n",
            "Evaluating:  75%|███████▍  | 1949/2613 [00:08<00:02, 231.67it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1974/2613 [00:08<00:02, 236.08it/s]\u001b[A\n",
            "Evaluating:  76%|███████▋  | 1998/2613 [00:08<00:02, 230.94it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2023/2613 [00:08<00:02, 235.50it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2048/2613 [00:08<00:02, 238.04it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2072/2613 [00:08<00:02, 238.49it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2096/2613 [00:08<00:02, 225.59it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2119/2613 [00:09<00:02, 223.28it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2144/2613 [00:09<00:02, 229.24it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2169/2613 [00:09<00:01, 233.94it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2193/2613 [00:09<00:01, 234.93it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2217/2613 [00:09<00:01, 228.63it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2240/2613 [00:09<00:01, 202.72it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2261/2613 [00:09<00:01, 194.73it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2281/2613 [00:09<00:01, 183.56it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2300/2613 [00:10<00:01, 177.41it/s]\u001b[A\n",
            "Evaluating:  89%|████████▊ | 2319/2613 [00:10<00:01, 179.75it/s]\u001b[A\n",
            "Evaluating:  89%|████████▉ | 2338/2613 [00:10<00:01, 179.91it/s]\u001b[A\n",
            "Evaluating:  90%|█████████ | 2357/2613 [00:10<00:01, 181.25it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2376/2613 [00:10<00:01, 182.35it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2395/2613 [00:10<00:01, 180.44it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2414/2613 [00:10<00:01, 175.99it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2432/2613 [00:10<00:01, 175.70it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2450/2613 [00:10<00:00, 171.68it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2468/2613 [00:10<00:00, 168.24it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▌| 2485/2613 [00:11<00:00, 161.30it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2502/2613 [00:11<00:00, 159.66it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▋| 2518/2613 [00:11<00:00, 159.69it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2534/2613 [00:11<00:00, 159.51it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2551/2613 [00:11<00:00, 160.71it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2568/2613 [00:11<00:00, 159.58it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▉| 2584/2613 [00:11<00:00, 159.38it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:11<00:00, 219.01it/s]\n",
            "Training | Loss: 0.23171235620975494 | Acc: 0.938144326210022: 100%|██████████| 10449/10449 [02:22<00:00, 73.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23267366239709092\n",
            "Test average accuracy: 0.9374894587600363\n",
            "\n",
            "Epoch 6 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.1455586552619934 | Acc: 0.969072163105011: 100%|█████████▉| 10446/10449 [02:09<00:00, 86.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.19902877509593964\n",
            "Train Average Accuracy: 0.9380090886780276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 24/2613 [00:00<00:11, 232.77it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 49/2613 [00:00<00:10, 239.05it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 74/2613 [00:00<00:10, 242.33it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 99/2613 [00:00<00:10, 244.69it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 124/2613 [00:00<00:10, 237.84it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 149/2613 [00:00<00:10, 241.18it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 174/2613 [00:00<00:10, 241.75it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 199/2613 [00:00<00:11, 209.09it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 221/2613 [00:01<00:13, 183.74it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 241/2613 [00:01<00:13, 181.30it/s]\u001b[A\n",
            "Evaluating:  10%|▉         | 260/2613 [00:01<00:13, 178.37it/s]\u001b[A\n",
            "Evaluating:  11%|█         | 279/2613 [00:01<00:13, 176.70it/s]\u001b[A\n",
            "Evaluating:  11%|█▏        | 297/2613 [00:01<00:13, 176.67it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 315/2613 [00:01<00:13, 176.05it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 334/2613 [00:01<00:12, 178.60it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 352/2613 [00:01<00:12, 174.82it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 371/2613 [00:01<00:12, 176.77it/s]\u001b[A\n",
            "Evaluating:  15%|█▍        | 389/2613 [00:01<00:12, 177.33it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 407/2613 [00:02<00:12, 176.61it/s]\u001b[A\n",
            "Evaluating:  16%|█▋        | 425/2613 [00:02<00:12, 173.02it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 443/2613 [00:02<00:12, 169.77it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 461/2613 [00:02<00:12, 171.51it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 479/2613 [00:02<00:12, 170.01it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 497/2613 [00:02<00:13, 162.53it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 514/2613 [00:02<00:12, 163.28it/s]\u001b[A\n",
            "Evaluating:  20%|██        | 531/2613 [00:02<00:12, 163.41it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 548/2613 [00:02<00:12, 163.70it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 565/2613 [00:03<00:13, 153.03it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 582/2613 [00:03<00:12, 156.43it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 598/2613 [00:03<00:12, 157.25it/s]\u001b[A\n",
            "Evaluating:  24%|██▎       | 616/2613 [00:03<00:12, 161.55it/s]\u001b[A\n",
            "Evaluating:  24%|██▍       | 634/2613 [00:03<00:11, 165.10it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 651/2613 [00:03<00:11, 163.90it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 668/2613 [00:03<00:12, 159.06it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 685/2613 [00:03<00:11, 161.75it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 702/2613 [00:03<00:11, 163.34it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 719/2613 [00:04<00:11, 164.16it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 736/2613 [00:04<00:11, 158.82it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 753/2613 [00:04<00:11, 160.99it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 770/2613 [00:04<00:11, 161.32it/s]\u001b[A\n",
            "Evaluating:  30%|███       | 787/2613 [00:04<00:11, 159.46it/s]\u001b[A\n",
            "Evaluating:  31%|███       | 809/2613 [00:04<00:10, 176.44it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 833/2613 [00:04<00:09, 191.81it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 854/2613 [00:04<00:08, 195.69it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 878/2613 [00:04<00:08, 206.75it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 901/2613 [00:04<00:08, 212.41it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 923/2613 [00:05<00:08, 209.38it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 947/2613 [00:05<00:07, 217.50it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 971/2613 [00:05<00:07, 221.53it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 996/2613 [00:05<00:07, 228.62it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1020/2613 [00:05<00:06, 230.43it/s]\u001b[A\n",
            "Evaluating:  40%|███▉      | 1044/2613 [00:05<00:06, 231.81it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1069/2613 [00:05<00:06, 233.44it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1093/2613 [00:05<00:06, 230.54it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1118/2613 [00:05<00:06, 235.23it/s]\u001b[A\n",
            "Evaluating:  44%|████▎     | 1142/2613 [00:06<00:06, 225.10it/s]\u001b[A\n",
            "Evaluating:  45%|████▍     | 1165/2613 [00:06<00:06, 223.72it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1189/2613 [00:06<00:06, 227.23it/s]\u001b[A\n",
            "Evaluating:  46%|████▋     | 1213/2613 [00:06<00:06, 230.88it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1238/2613 [00:06<00:05, 234.52it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1263/2613 [00:06<00:05, 238.42it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1287/2613 [00:06<00:05, 236.56it/s]\u001b[A\n",
            "Evaluating:  50%|█████     | 1311/2613 [00:06<00:05, 232.88it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1335/2613 [00:06<00:05, 232.93it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1359/2613 [00:06<00:05, 233.74it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1383/2613 [00:07<00:05, 221.56it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1408/2613 [00:07<00:05, 228.26it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1433/2613 [00:07<00:05, 231.89it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1458/2613 [00:07<00:04, 234.68it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1483/2613 [00:07<00:04, 237.81it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1507/2613 [00:07<00:04, 238.37it/s]\u001b[A\n",
            "Evaluating:  59%|█████▊    | 1532/2613 [00:07<00:04, 239.44it/s]\u001b[A\n",
            "Evaluating:  60%|█████▉    | 1556/2613 [00:07<00:04, 232.89it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1581/2613 [00:07<00:04, 235.20it/s]\u001b[A\n",
            "Evaluating:  61%|██████▏   | 1605/2613 [00:08<00:04, 227.41it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1628/2613 [00:08<00:04, 225.08it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1652/2613 [00:08<00:04, 228.83it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1676/2613 [00:08<00:04, 231.41it/s]\u001b[A\n",
            "Evaluating:  65%|██████▌   | 1702/2613 [00:08<00:03, 236.98it/s]\u001b[A\n",
            "Evaluating:  66%|██████▌   | 1726/2613 [00:08<00:03, 237.58it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1751/2613 [00:08<00:03, 240.29it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1776/2613 [00:08<00:03, 241.93it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1801/2613 [00:08<00:03, 237.29it/s]\u001b[A\n",
            "Evaluating:  70%|██████▉   | 1825/2613 [00:08<00:03, 237.72it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1849/2613 [00:09<00:03, 223.35it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1873/2613 [00:09<00:03, 227.27it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1898/2613 [00:09<00:03, 231.39it/s]\u001b[A\n",
            "Evaluating:  74%|███████▎  | 1923/2613 [00:09<00:02, 234.11it/s]\u001b[A\n",
            "Evaluating:  75%|███████▍  | 1948/2613 [00:09<00:02, 236.29it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1974/2613 [00:09<00:02, 241.40it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 1999/2613 [00:09<00:02, 240.61it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2024/2613 [00:09<00:02, 241.53it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2049/2613 [00:09<00:02, 235.12it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2073/2613 [00:10<00:02, 227.37it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2096/2613 [00:10<00:02, 225.50it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2120/2613 [00:10<00:02, 229.00it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2145/2613 [00:10<00:02, 232.82it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2170/2613 [00:10<00:01, 237.59it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2195/2613 [00:10<00:01, 241.22it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2220/2613 [00:10<00:01, 239.72it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2245/2613 [00:10<00:01, 240.89it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2270/2613 [00:10<00:01, 237.21it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2294/2613 [00:10<00:01, 234.76it/s]\u001b[A\n",
            "Evaluating:  89%|████████▊ | 2318/2613 [00:11<00:01, 222.80it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2343/2613 [00:11<00:01, 229.58it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2367/2613 [00:11<00:01, 230.56it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2391/2613 [00:11<00:00, 232.34it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2416/2613 [00:11<00:00, 235.95it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2441/2613 [00:11<00:00, 238.90it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2466/2613 [00:11<00:00, 239.48it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▌| 2491/2613 [00:11<00:00, 240.95it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▋| 2516/2613 [00:11<00:00, 236.51it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2540/2613 [00:11<00:00, 228.27it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2563/2613 [00:12<00:00, 224.53it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▉| 2588/2613 [00:12<00:00, 230.00it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:12<00:00, 211.94it/s]\n",
            "Training | Loss: 0.1455586552619934 | Acc: 0.969072163105011: 100%|██████████| 10449/10449 [02:22<00:00, 73.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23246304229346673\n",
            "Test average accuracy: 0.9374894583722526\n",
            "\n",
            "Epoch 7 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.23328709602355957 | Acc: 0.938144326210022: 100%|█████████▉| 10443/10449 [02:10<00:00, 85.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.25361698865890503\n",
            "Train Average Accuracy: 0.9380229108357758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 23/2613 [00:00<00:11, 229.62it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 48/2613 [00:00<00:10, 235.17it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 73/2613 [00:00<00:10, 237.95it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 98/2613 [00:00<00:10, 239.56it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 122/2613 [00:00<00:10, 235.75it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 146/2613 [00:00<00:10, 231.35it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 170/2613 [00:00<00:10, 223.58it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 195/2613 [00:00<00:10, 230.51it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 220/2613 [00:00<00:10, 235.94it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 245/2613 [00:01<00:09, 239.70it/s]\u001b[A\n",
            "Evaluating:  10%|█         | 271/2613 [00:01<00:09, 242.97it/s]\u001b[A\n",
            "Evaluating:  11%|█▏        | 296/2613 [00:01<00:09, 241.12it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 322/2613 [00:01<00:09, 245.52it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 347/2613 [00:01<00:09, 246.32it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 372/2613 [00:01<00:09, 240.59it/s]\u001b[A\n",
            "Evaluating:  15%|█▌        | 397/2613 [00:01<00:09, 227.57it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 421/2613 [00:01<00:09, 228.73it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 446/2613 [00:01<00:09, 233.68it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 471/2613 [00:01<00:09, 236.57it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 495/2613 [00:02<00:09, 233.82it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 519/2613 [00:02<00:08, 234.29it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 544/2613 [00:02<00:08, 236.41it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 569/2613 [00:02<00:08, 239.24it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 594/2613 [00:02<00:08, 239.87it/s]\u001b[A\n",
            "Evaluating:  24%|██▎       | 619/2613 [00:02<00:08, 228.34it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 642/2613 [00:02<00:08, 222.17it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 667/2613 [00:02<00:08, 227.93it/s]\u001b[A\n",
            "Evaluating:  26%|██▋       | 692/2613 [00:02<00:08, 231.75it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 717/2613 [00:03<00:08, 235.50it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 742/2613 [00:03<00:07, 237.32it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 767/2613 [00:03<00:07, 239.14it/s]\u001b[A\n",
            "Evaluating:  30%|███       | 792/2613 [00:03<00:07, 240.78it/s]\u001b[A\n",
            "Evaluating:  31%|███▏      | 817/2613 [00:03<00:07, 241.58it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 842/2613 [00:03<00:07, 235.93it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 866/2613 [00:03<00:07, 223.57it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 890/2613 [00:03<00:07, 228.00it/s]\u001b[A\n",
            "Evaluating:  35%|███▍      | 914/2613 [00:03<00:07, 230.97it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 938/2613 [00:04<00:07, 229.18it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 961/2613 [00:04<00:07, 224.15it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 984/2613 [00:04<00:07, 213.56it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 1006/2613 [00:04<00:08, 199.13it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1027/2613 [00:04<00:08, 191.36it/s]\u001b[A\n",
            "Evaluating:  40%|████      | 1047/2613 [00:04<00:08, 183.70it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1066/2613 [00:04<00:09, 170.86it/s]\u001b[A\n",
            "Evaluating:  41%|████▏     | 1084/2613 [00:04<00:09, 168.27it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1101/2613 [00:04<00:09, 167.02it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1119/2613 [00:05<00:08, 170.45it/s]\u001b[A\n",
            "Evaluating:  44%|████▎     | 1137/2613 [00:05<00:08, 171.47it/s]\u001b[A\n",
            "Evaluating:  44%|████▍     | 1155/2613 [00:05<00:08, 173.55it/s]\u001b[A\n",
            "Evaluating:  45%|████▍     | 1174/2613 [00:05<00:08, 175.79it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1193/2613 [00:05<00:07, 177.77it/s]\u001b[A\n",
            "Evaluating:  46%|████▋     | 1211/2613 [00:05<00:08, 169.17it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1229/2613 [00:05<00:08, 157.33it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1246/2613 [00:05<00:08, 159.62it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1263/2613 [00:05<00:08, 155.44it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1279/2613 [00:06<00:08, 155.96it/s]\u001b[A\n",
            "Evaluating:  50%|████▉     | 1295/2613 [00:06<00:08, 156.82it/s]\u001b[A\n",
            "Evaluating:  50%|█████     | 1313/2613 [00:06<00:08, 160.95it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1331/2613 [00:06<00:07, 164.15it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1348/2613 [00:06<00:07, 164.16it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1366/2613 [00:06<00:07, 166.48it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1383/2613 [00:06<00:07, 160.26it/s]\u001b[A\n",
            "Evaluating:  54%|█████▎    | 1400/2613 [00:06<00:07, 158.27it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1417/2613 [00:06<00:07, 160.96it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1434/2613 [00:06<00:07, 161.45it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1451/2613 [00:07<00:07, 162.16it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1468/2613 [00:07<00:07, 161.44it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1485/2613 [00:07<00:06, 161.19it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1502/2613 [00:07<00:07, 157.82it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1518/2613 [00:07<00:07, 153.97it/s]\u001b[A\n",
            "Evaluating:  59%|█████▊    | 1534/2613 [00:07<00:07, 152.11it/s]\u001b[A\n",
            "Evaluating:  59%|█████▉    | 1550/2613 [00:07<00:07, 145.19it/s]\u001b[A\n",
            "Evaluating:  60%|██████    | 1568/2613 [00:07<00:06, 154.01it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1591/2613 [00:07<00:05, 174.74it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1614/2613 [00:08<00:05, 190.25it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1638/2613 [00:08<00:04, 203.42it/s]\u001b[A\n",
            "Evaluating:  64%|██████▎   | 1661/2613 [00:08<00:04, 210.40it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1685/2613 [00:08<00:04, 218.36it/s]\u001b[A\n",
            "Evaluating:  65%|██████▌   | 1710/2613 [00:08<00:04, 225.44it/s]\u001b[A\n",
            "Evaluating:  66%|██████▋   | 1734/2613 [00:08<00:03, 229.06it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1757/2613 [00:08<00:03, 218.76it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1780/2613 [00:08<00:03, 213.73it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1804/2613 [00:08<00:03, 220.68it/s]\u001b[A\n",
            "Evaluating:  70%|██████▉   | 1829/2613 [00:09<00:03, 227.46it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1853/2613 [00:09<00:03, 230.08it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1877/2613 [00:09<00:03, 232.35it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1901/2613 [00:09<00:03, 234.14it/s]\u001b[A\n",
            "Evaluating:  74%|███████▎  | 1925/2613 [00:09<00:02, 231.69it/s]\u001b[A\n",
            "Evaluating:  75%|███████▍  | 1949/2613 [00:09<00:02, 233.47it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1973/2613 [00:09<00:02, 228.04it/s]\u001b[A\n",
            "Evaluating:  76%|███████▋  | 1996/2613 [00:09<00:02, 221.11it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2019/2613 [00:09<00:02, 221.01it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2044/2613 [00:09<00:02, 227.69it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2069/2613 [00:10<00:02, 231.63it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2094/2613 [00:10<00:02, 235.47it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2119/2613 [00:10<00:02, 237.98it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2144/2613 [00:10<00:01, 240.64it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2169/2613 [00:10<00:01, 243.20it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2194/2613 [00:10<00:01, 243.85it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2219/2613 [00:10<00:01, 229.24it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2243/2613 [00:10<00:01, 228.99it/s]\u001b[A\n",
            "Training | Loss: 0.23328709602355957 | Acc: 0.938144326210022: 100%|█████████▉| 10443/10449 [02:21<00:00, 85.84it/s]\n",
            "Evaluating:  88%|████████▊ | 2290/2613 [00:10<00:01, 226.18it/s]\u001b[A\n",
            "Evaluating:  89%|████████▊ | 2315/2613 [00:11<00:01, 230.41it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2340/2613 [00:11<00:01, 233.73it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2365/2613 [00:11<00:01, 236.74it/s]\u001b[A\n",
            "Evaluating:  91%|█████████▏| 2390/2613 [00:11<00:00, 239.78it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2415/2613 [00:11<00:00, 242.53it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2440/2613 [00:11<00:00, 236.68it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2464/2613 [00:11<00:00, 225.65it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▌| 2489/2613 [00:11<00:00, 229.49it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2513/2613 [00:11<00:00, 231.46it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2538/2613 [00:12<00:00, 235.26it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2563/2613 [00:12<00:00, 238.56it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▉| 2587/2613 [00:12<00:00, 238.63it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:12<00:00, 211.21it/s]\n",
            "Training | Loss: 0.23328709602355957 | Acc: 0.938144326210022: 100%|██████████| 10449/10449 [02:23<00:00, 72.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23267056756477178\n",
            "Test average accuracy: 0.9376119232433215\n",
            "\n",
            "Epoch 8 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.4744303822517395 | Acc: 0.8556700944900513: 100%|█████████▉| 10447/10449 [02:10<00:00, 88.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.17469371855258942\n",
            "Train Average Accuracy: 0.9380343245276952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 23/2613 [00:00<00:11, 228.41it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 48/2613 [00:00<00:10, 240.10it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 73/2613 [00:00<00:10, 241.83it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 98/2613 [00:00<00:11, 226.26it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 121/2613 [00:00<00:11, 219.02it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 144/2613 [00:00<00:11, 216.11it/s]\u001b[A\n",
            "Evaluating:   6%|▋         | 169/2613 [00:00<00:10, 225.34it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 195/2613 [00:00<00:10, 233.76it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 219/2613 [00:00<00:10, 235.54it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 243/2613 [00:01<00:10, 232.53it/s]\u001b[A\n",
            "Evaluating:  10%|█         | 267/2613 [00:01<00:10, 223.15it/s]\u001b[A\n",
            "Evaluating:  11%|█         | 291/2613 [00:01<00:10, 227.67it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 314/2613 [00:01<00:10, 217.82it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 336/2613 [00:01<00:10, 218.40it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 360/2613 [00:01<00:10, 224.61it/s]\u001b[A\n",
            "Evaluating:  15%|█▍        | 383/2613 [00:01<00:09, 226.09it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 407/2613 [00:01<00:09, 228.02it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 432/2613 [00:01<00:09, 233.84it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 456/2613 [00:01<00:09, 234.89it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 481/2613 [00:02<00:08, 237.63it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 505/2613 [00:02<00:08, 236.71it/s]\u001b[A\n",
            "Evaluating:  20%|██        | 529/2613 [00:02<00:08, 234.16it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 553/2613 [00:02<00:09, 224.75it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 578/2613 [00:02<00:08, 230.76it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 602/2613 [00:02<00:08, 233.26it/s]\u001b[A\n",
            "Evaluating:  24%|██▍       | 626/2613 [00:02<00:08, 230.79it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 651/2613 [00:02<00:08, 235.30it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 675/2613 [00:02<00:08, 235.42it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 700/2613 [00:03<00:08, 239.03it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 724/2613 [00:03<00:07, 239.20it/s]\u001b[A\n",
            "Evaluating:  29%|██▊       | 748/2613 [00:03<00:07, 239.12it/s]\u001b[A\n",
            "Evaluating:  30%|██▉       | 772/2613 [00:03<00:07, 231.18it/s]\u001b[A\n",
            "Evaluating:  30%|███       | 796/2613 [00:03<00:08, 223.88it/s]\u001b[A\n",
            "Evaluating:  31%|███▏      | 820/2613 [00:03<00:07, 228.02it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 843/2613 [00:03<00:07, 227.12it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 867/2613 [00:03<00:07, 229.02it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 892/2613 [00:03<00:07, 233.98it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 917/2613 [00:03<00:07, 237.90it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 941/2613 [00:04<00:07, 237.40it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 965/2613 [00:04<00:06, 236.79it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 990/2613 [00:04<00:06, 238.64it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1014/2613 [00:04<00:07, 224.24it/s]\u001b[A\n",
            "Evaluating:  40%|███▉      | 1039/2613 [00:04<00:06, 230.32it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1063/2613 [00:04<00:06, 232.66it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1087/2613 [00:04<00:06, 229.59it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1111/2613 [00:04<00:06, 232.47it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1136/2613 [00:04<00:06, 236.48it/s]\u001b[A\n",
            "Evaluating:  44%|████▍     | 1160/2613 [00:05<00:06, 235.09it/s]\u001b[A\n",
            "Evaluating:  45%|████▌     | 1184/2613 [00:05<00:06, 234.09it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1208/2613 [00:05<00:05, 234.63it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1232/2613 [00:05<00:05, 230.75it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1256/2613 [00:05<00:06, 222.81it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1280/2613 [00:05<00:05, 227.55it/s]\u001b[A\n",
            "Evaluating:  50%|████▉     | 1304/2613 [00:05<00:05, 229.60it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1328/2613 [00:05<00:05, 227.35it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1353/2613 [00:05<00:05, 232.24it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1377/2613 [00:05<00:05, 232.65it/s]\u001b[A\n",
            "Evaluating:  54%|█████▎    | 1401/2613 [00:06<00:05, 233.88it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1425/2613 [00:06<00:05, 232.52it/s]\u001b[A\n",
            "Evaluating:  55%|█████▌    | 1450/2613 [00:06<00:04, 235.34it/s]\u001b[A\n",
            "Evaluating:  56%|█████▋    | 1474/2613 [00:06<00:05, 221.94it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1497/2613 [00:06<00:04, 223.51it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1520/2613 [00:06<00:04, 224.03it/s]\u001b[A\n",
            "Evaluating:  59%|█████▉    | 1545/2613 [00:06<00:04, 229.51it/s]\u001b[A\n",
            "Evaluating:  60%|██████    | 1569/2613 [00:06<00:04, 226.31it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1594/2613 [00:06<00:04, 231.25it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1618/2613 [00:07<00:04, 233.19it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1642/2613 [00:07<00:04, 234.52it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1667/2613 [00:07<00:03, 238.23it/s]\u001b[A\n",
            "Evaluating:  65%|██████▍   | 1691/2613 [00:07<00:04, 230.27it/s]\u001b[A\n",
            "Evaluating:  66%|██████▌   | 1715/2613 [00:07<00:03, 225.99it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1738/2613 [00:07<00:03, 226.78it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1763/2613 [00:07<00:03, 231.52it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1788/2613 [00:07<00:03, 235.14it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1812/2613 [00:07<00:03, 212.63it/s]\u001b[A\n",
            "Evaluating:  70%|███████   | 1834/2613 [00:08<00:03, 197.82it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1855/2613 [00:08<00:04, 187.14it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1875/2613 [00:08<00:04, 179.01it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1894/2613 [00:08<00:04, 174.86it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1912/2613 [00:08<00:04, 168.53it/s]\u001b[A\n",
            "Evaluating:  74%|███████▍  | 1929/2613 [00:08<00:04, 168.50it/s]\u001b[A\n",
            "Evaluating:  75%|███████▍  | 1947/2613 [00:08<00:03, 170.09it/s]\u001b[A\n",
            "Evaluating:  75%|███████▌  | 1965/2613 [00:08<00:03, 166.91it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1982/2613 [00:08<00:03, 164.99it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2000/2613 [00:09<00:03, 167.41it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2017/2613 [00:09<00:03, 164.77it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2034/2613 [00:09<00:03, 163.59it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2051/2613 [00:09<00:03, 150.67it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2067/2613 [00:09<00:03, 150.45it/s]\u001b[A\n",
            "Evaluating:  80%|███████▉  | 2083/2613 [00:09<00:03, 149.85it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2099/2613 [00:09<00:03, 147.86it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2115/2613 [00:09<00:03, 150.87it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2131/2613 [00:09<00:03, 147.21it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2148/2613 [00:10<00:03, 152.35it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2165/2613 [00:10<00:02, 155.13it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2181/2613 [00:10<00:02, 153.40it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2198/2613 [00:10<00:02, 155.36it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2214/2613 [00:10<00:02, 151.17it/s]\u001b[A\n",
            "Evaluating:  85%|████████▌ | 2230/2613 [00:10<00:02, 149.71it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2246/2613 [00:10<00:02, 151.46it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2263/2613 [00:10<00:02, 156.41it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2280/2613 [00:10<00:02, 158.06it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2296/2613 [00:10<00:02, 153.10it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2312/2613 [00:11<00:01, 154.95it/s]\u001b[A\n",
            "Evaluating:  89%|████████▉ | 2328/2613 [00:11<00:01, 151.01it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2344/2613 [00:11<00:01, 152.32it/s]\u001b[A\n",
            "Evaluating:  90%|█████████ | 2363/2613 [00:11<00:01, 162.61it/s]\u001b[A\n",
            "Evaluating:  91%|█████████▏| 2385/2613 [00:11<00:01, 178.34it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2407/2613 [00:11<00:01, 189.08it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2430/2613 [00:11<00:00, 199.52it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2454/2613 [00:11<00:00, 209.54it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▍| 2478/2613 [00:11<00:00, 217.30it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2501/2613 [00:12<00:00, 219.91it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2524/2613 [00:12<00:00, 218.88it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2548/2613 [00:12<00:00, 223.87it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2571/2613 [00:12<00:00, 222.51it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:12<00:00, 208.60it/s]\n",
            "Training | Loss: 0.4744303822517395 | Acc: 0.8556700944900513: 100%|██████████| 10449/10449 [02:23<00:00, 72.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2324314154898395\n",
            "Test average accuracy: 0.9375239013713117\n",
            "\n",
            "Epoch 9 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.17806263267993927 | Acc: 0.9587628841400146: 100%|█████████▉| 10443/10449 [02:10<00:00, 58.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3044670522212982\n",
            "Train Average Accuracy: 0.9380403636767046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   0%|          | 11/2613 [00:00<00:23, 109.55it/s]\u001b[A\n",
            "Evaluating:   1%|          | 26/2613 [00:00<00:19, 132.70it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 42/2613 [00:00<00:17, 144.93it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 58/2613 [00:00<00:17, 149.34it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 74/2613 [00:00<00:16, 150.50it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 90/2613 [00:00<00:16, 149.57it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 106/2613 [00:00<00:16, 150.19it/s]\u001b[A\n",
            "Evaluating:   5%|▍         | 122/2613 [00:00<00:16, 150.37it/s]\u001b[A\n",
            "Evaluating:   5%|▌         | 138/2613 [00:00<00:16, 148.42it/s]\u001b[A\n",
            "Evaluating:   6%|▌         | 162/2613 [00:01<00:14, 173.81it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 181/2613 [00:01<00:13, 177.90it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 203/2613 [00:01<00:12, 188.47it/s]\u001b[A\n",
            "Evaluating:   9%|▊         | 227/2613 [00:01<00:11, 201.80it/s]\u001b[A\n",
            "Evaluating:  10%|▉         | 251/2613 [00:01<00:11, 212.97it/s]\u001b[A\n",
            "Evaluating:  11%|█         | 275/2613 [00:01<00:10, 220.46it/s]\u001b[A\n",
            "Evaluating:  11%|█▏        | 299/2613 [00:01<00:10, 226.19it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 323/2613 [00:01<00:09, 230.10it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 347/2613 [00:01<00:09, 231.73it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 371/2613 [00:01<00:09, 227.03it/s]\u001b[A\n",
            "Evaluating:  15%|█▌        | 395/2613 [00:02<00:09, 229.73it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 419/2613 [00:02<00:10, 218.04it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 443/2613 [00:02<00:09, 223.39it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 467/2613 [00:02<00:09, 226.91it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 492/2613 [00:02<00:09, 233.28it/s]\u001b[A\n",
            "Evaluating:  20%|█▉        | 516/2613 [00:02<00:08, 234.53it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 542/2613 [00:02<00:08, 240.46it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 567/2613 [00:02<00:08, 241.84it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 593/2613 [00:02<00:08, 245.11it/s]\u001b[A\n",
            "Evaluating:  24%|██▎       | 618/2613 [00:03<00:08, 236.54it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 642/2613 [00:03<00:08, 227.49it/s]\u001b[A\n",
            "Evaluating:  25%|██▌       | 665/2613 [00:03<00:08, 225.60it/s]\u001b[A\n",
            "Evaluating:  26%|██▋       | 689/2613 [00:03<00:08, 229.06it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 714/2613 [00:03<00:08, 233.93it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 738/2613 [00:03<00:08, 234.25it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 763/2613 [00:03<00:07, 237.72it/s]\u001b[A\n",
            "Evaluating:  30%|███       | 787/2613 [00:03<00:07, 237.03it/s]\u001b[A\n",
            "Evaluating:  31%|███       | 812/2613 [00:03<00:07, 240.07it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 837/2613 [00:03<00:07, 239.36it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 861/2613 [00:04<00:07, 230.85it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 885/2613 [00:04<00:08, 213.80it/s]\u001b[A\n",
            "Evaluating:  35%|███▍      | 908/2613 [00:04<00:07, 217.91it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 933/2613 [00:04<00:07, 225.01it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 958/2613 [00:04<00:07, 231.86it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 983/2613 [00:04<00:06, 235.66it/s]\u001b[A\n",
            "Evaluating:  39%|███▊      | 1009/2613 [00:04<00:06, 240.57it/s]\u001b[A\n",
            "Evaluating:  40%|███▉      | 1034/2613 [00:04<00:06, 240.45it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1060/2613 [00:04<00:06, 243.78it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1085/2613 [00:05<00:06, 236.10it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1109/2613 [00:05<00:06, 226.80it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1132/2613 [00:05<00:06, 226.48it/s]\u001b[A\n",
            "Evaluating:  44%|████▍     | 1156/2613 [00:05<00:06, 229.91it/s]\u001b[A\n",
            "Evaluating:  45%|████▌     | 1180/2613 [00:05<00:06, 232.27it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1204/2613 [00:05<00:06, 234.51it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1228/2613 [00:05<00:05, 233.52it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1252/2613 [00:05<00:05, 233.93it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1276/2613 [00:05<00:05, 232.41it/s]\u001b[A\n",
            "Evaluating:  50%|████▉     | 1301/2613 [00:05<00:05, 235.90it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1325/2613 [00:06<00:05, 229.42it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1348/2613 [00:06<00:05, 218.24it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1372/2613 [00:06<00:05, 223.32it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1396/2613 [00:06<00:05, 227.42it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1421/2613 [00:06<00:05, 231.72it/s]\u001b[A\n",
            "Evaluating:  55%|█████▌    | 1445/2613 [00:06<00:04, 233.66it/s]\u001b[A\n",
            "Evaluating:  56%|█████▋    | 1470/2613 [00:06<00:04, 235.80it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1494/2613 [00:06<00:04, 236.68it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1518/2613 [00:06<00:04, 237.14it/s]\u001b[A\n",
            "Evaluating:  59%|█████▉    | 1543/2613 [00:07<00:04, 239.03it/s]\u001b[A\n",
            "Evaluating:  60%|█████▉    | 1567/2613 [00:07<00:04, 224.95it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1590/2613 [00:07<00:04, 222.51it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1615/2613 [00:07<00:04, 227.98it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1639/2613 [00:07<00:04, 231.15it/s]\u001b[A\n",
            "Evaluating:  64%|██████▎   | 1664/2613 [00:07<00:04, 235.46it/s]\u001b[A\n",
            "Evaluating:  65%|██████▍   | 1689/2613 [00:07<00:03, 238.10it/s]\u001b[A\n",
            "Evaluating:  66%|██████▌   | 1714/2613 [00:07<00:03, 239.23it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1739/2613 [00:07<00:03, 240.74it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1764/2613 [00:07<00:03, 239.39it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1789/2613 [00:08<00:03, 240.91it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1814/2613 [00:08<00:03, 220.73it/s]\u001b[A\n",
            "Evaluating:  70%|███████   | 1838/2613 [00:08<00:03, 225.64it/s]\u001b[A\n",
            "Evaluating:  71%|███████   | 1861/2613 [00:08<00:03, 225.79it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1884/2613 [00:08<00:03, 215.49it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1906/2613 [00:08<00:03, 216.53it/s]\u001b[A\n",
            "Evaluating:  74%|███████▍  | 1930/2613 [00:08<00:03, 221.81it/s]\u001b[A\n",
            "Evaluating:  75%|███████▍  | 1954/2613 [00:08<00:02, 226.09it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1979/2613 [00:08<00:02, 230.74it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2003/2613 [00:09<00:02, 233.35it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2027/2613 [00:09<00:02, 220.48it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2050/2613 [00:09<00:02, 217.58it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2074/2613 [00:09<00:02, 222.25it/s]\u001b[A\n",
            "Evaluating:  80%|████████  | 2099/2613 [00:09<00:02, 228.39it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2122/2613 [00:09<00:02, 227.12it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2146/2613 [00:09<00:02, 228.63it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2169/2613 [00:09<00:01, 226.86it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2193/2613 [00:09<00:01, 229.78it/s]\u001b[A\n",
            "Evaluating:  85%|████████▍ | 2217/2613 [00:09<00:01, 231.75it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2241/2613 [00:10<00:01, 231.70it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2265/2613 [00:10<00:01, 215.28it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2290/2613 [00:10<00:01, 223.14it/s]\u001b[A\n",
            "Evaluating:  89%|████████▊ | 2314/2613 [00:10<00:01, 225.52it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2339/2613 [00:10<00:01, 230.19it/s]\u001b[A\n",
            "Evaluating:  90%|█████████ | 2363/2613 [00:10<00:01, 231.57it/s]\u001b[A\n",
            "Evaluating:  91%|█████████▏| 2387/2613 [00:10<00:00, 232.76it/s]\u001b[A\n",
            "Evaluating:  92%|█████████▏| 2411/2613 [00:10<00:00, 234.51it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2436/2613 [00:10<00:00, 237.24it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2460/2613 [00:11<00:00, 215.06it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▍| 2482/2613 [00:11<00:00, 189.60it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2502/2613 [00:11<00:00, 180.52it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▋| 2521/2613 [00:11<00:00, 177.74it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2540/2613 [00:11<00:00, 178.24it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2559/2613 [00:11<00:00, 177.38it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▊| 2577/2613 [00:11<00:00, 175.28it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▉| 2595/2613 [00:11<00:00, 175.74it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:11<00:00, 217.99it/s]\n",
            "Training | Loss: 0.17806263267993927 | Acc: 0.9587628841400146: 100%|██████████| 10449/10449 [02:22<00:00, 73.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.23279125588723995\n",
            "Test average accuracy: 0.9375621721838517\n",
            "\n",
            "Epoch 10 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Loss: 0.20329102873802185 | Acc: 0.9484536051750183: 100%|█████████▉| 10443/10449 [02:10<00:00, 87.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.17228034138679504\n",
            "Train Average Accuracy: 0.9380476802996796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2613 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   1%|          | 23/2613 [00:00<00:11, 226.15it/s]\u001b[A\n",
            "Evaluating:   2%|▏         | 46/2613 [00:00<00:11, 226.57it/s]\u001b[A\n",
            "Evaluating:   3%|▎         | 69/2613 [00:00<00:11, 225.41it/s]\u001b[A\n",
            "Evaluating:   4%|▎         | 92/2613 [00:00<00:11, 221.35it/s]\u001b[A\n",
            "Evaluating:   4%|▍         | 116/2613 [00:00<00:10, 227.78it/s]\u001b[A\n",
            "Evaluating:   5%|▌         | 141/2613 [00:00<00:10, 232.62it/s]\u001b[A\n",
            "Evaluating:   6%|▋         | 166/2613 [00:00<00:10, 237.03it/s]\u001b[A\n",
            "Evaluating:   7%|▋         | 191/2613 [00:00<00:10, 239.30it/s]\u001b[A\n",
            "Evaluating:   8%|▊         | 216/2613 [00:00<00:09, 242.56it/s]\u001b[A\n",
            "Evaluating:   9%|▉         | 241/2613 [00:01<00:09, 242.89it/s]\u001b[A\n",
            "Evaluating:  10%|█         | 266/2613 [00:01<00:09, 244.23it/s]\u001b[A\n",
            "Evaluating:  11%|█         | 291/2613 [00:01<00:09, 233.85it/s]\u001b[A\n",
            "Evaluating:  12%|█▏        | 315/2613 [00:01<00:10, 218.61it/s]\u001b[A\n",
            "Evaluating:  13%|█▎        | 338/2613 [00:01<00:10, 217.21it/s]\u001b[A\n",
            "Evaluating:  14%|█▍        | 363/2613 [00:01<00:09, 225.62it/s]\u001b[A\n",
            "Evaluating:  15%|█▍        | 386/2613 [00:01<00:10, 219.18it/s]\u001b[A\n",
            "Evaluating:  16%|█▌        | 409/2613 [00:01<00:10, 202.70it/s]\u001b[A\n",
            "Evaluating:  16%|█▋        | 430/2613 [00:01<00:11, 193.93it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 450/2613 [00:02<00:11, 190.37it/s]\u001b[A\n",
            "Evaluating:  18%|█▊        | 470/2613 [00:02<00:11, 187.42it/s]\u001b[A\n",
            "Evaluating:  19%|█▊        | 489/2613 [00:02<00:12, 176.03it/s]\u001b[A\n",
            "Evaluating:  19%|█▉        | 507/2613 [00:02<00:12, 168.84it/s]\u001b[A\n",
            "Evaluating:  20%|██        | 524/2613 [00:02<00:12, 163.51it/s]\u001b[A\n",
            "Evaluating:  21%|██        | 542/2613 [00:02<00:12, 166.63it/s]\u001b[A\n",
            "Evaluating:  21%|██▏       | 559/2613 [00:02<00:12, 166.85it/s]\u001b[A\n",
            "Evaluating:  22%|██▏       | 576/2613 [00:02<00:12, 167.00it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 594/2613 [00:02<00:11, 169.37it/s]\u001b[A\n",
            "Evaluating:  23%|██▎       | 612/2613 [00:03<00:11, 170.35it/s]\u001b[A\n",
            "Evaluating:  24%|██▍       | 630/2613 [00:03<00:11, 165.31it/s]\u001b[A\n",
            "Evaluating:  25%|██▍       | 647/2613 [00:03<00:12, 157.52it/s]\u001b[A\n",
            "Evaluating:  25%|██▌       | 664/2613 [00:03<00:12, 160.81it/s]\u001b[A\n",
            "Evaluating:  26%|██▌       | 681/2613 [00:03<00:12, 152.43it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 698/2613 [00:03<00:12, 155.11it/s]\u001b[A\n",
            "Evaluating:  27%|██▋       | 714/2613 [00:03<00:12, 156.27it/s]\u001b[A\n",
            "Evaluating:  28%|██▊       | 731/2613 [00:03<00:11, 160.06it/s]\u001b[A\n",
            "Evaluating:  29%|██▊       | 748/2613 [00:03<00:11, 162.04it/s]\u001b[A\n",
            "Evaluating:  29%|██▉       | 765/2613 [00:04<00:11, 163.86it/s]\u001b[A\n",
            "Evaluating:  30%|██▉       | 782/2613 [00:04<00:11, 164.12it/s]\u001b[A\n",
            "Evaluating:  31%|███       | 799/2613 [00:04<00:10, 165.53it/s]\u001b[A\n",
            "Evaluating:  31%|███       | 816/2613 [00:04<00:10, 163.95it/s]\u001b[A\n",
            "Evaluating:  32%|███▏      | 833/2613 [00:04<00:11, 160.97it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 850/2613 [00:04<00:11, 151.91it/s]\u001b[A\n",
            "Evaluating:  33%|███▎      | 866/2613 [00:04<00:11, 152.56it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 883/2613 [00:04<00:11, 156.80it/s]\u001b[A\n",
            "Evaluating:  34%|███▍      | 900/2613 [00:04<00:10, 158.89it/s]\u001b[A\n",
            "Evaluating:  35%|███▌      | 916/2613 [00:04<00:11, 154.16it/s]\u001b[A\n",
            "Evaluating:  36%|███▌      | 933/2613 [00:05<00:10, 156.65it/s]\u001b[A\n",
            "Evaluating:  36%|███▋      | 950/2613 [00:05<00:10, 159.35it/s]\u001b[A\n",
            "Evaluating:  37%|███▋      | 966/2613 [00:05<00:10, 149.83it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 982/2613 [00:05<00:10, 152.21it/s]\u001b[A\n",
            "Evaluating:  38%|███▊      | 1004/2613 [00:05<00:09, 171.33it/s]\u001b[A\n",
            "Evaluating:  39%|███▉      | 1025/2613 [00:05<00:08, 182.35it/s]\u001b[A\n",
            "Evaluating:  40%|████      | 1048/2613 [00:05<00:07, 196.24it/s]\u001b[A\n",
            "Evaluating:  41%|████      | 1072/2613 [00:05<00:07, 207.13it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 1096/2613 [00:05<00:07, 215.29it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 1120/2613 [00:06<00:06, 221.03it/s]\u001b[A\n",
            "Evaluating:  44%|████▍     | 1144/2613 [00:06<00:06, 225.52it/s]\u001b[A\n",
            "Evaluating:  45%|████▍     | 1167/2613 [00:06<00:06, 218.30it/s]\u001b[A\n",
            "Evaluating:  46%|████▌     | 1189/2613 [00:06<00:06, 214.85it/s]\u001b[A\n",
            "Evaluating:  46%|████▋     | 1213/2613 [00:06<00:06, 219.44it/s]\u001b[A\n",
            "Evaluating:  47%|████▋     | 1237/2613 [00:06<00:06, 224.54it/s]\u001b[A\n",
            "Evaluating:  48%|████▊     | 1260/2613 [00:06<00:06, 222.14it/s]\u001b[A\n",
            "Evaluating:  49%|████▉     | 1284/2613 [00:06<00:05, 226.82it/s]\u001b[A\n",
            "Evaluating:  50%|█████     | 1308/2613 [00:06<00:05, 230.67it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 1333/2613 [00:06<00:05, 233.90it/s]\u001b[A\n",
            "Evaluating:  52%|█████▏    | 1357/2613 [00:07<00:05, 234.11it/s]\u001b[A\n",
            "Evaluating:  53%|█████▎    | 1381/2613 [00:07<00:05, 235.66it/s]\u001b[A\n",
            "Evaluating:  54%|█████▍    | 1405/2613 [00:07<00:05, 226.00it/s]\u001b[A\n",
            "Evaluating:  55%|█████▍    | 1428/2613 [00:07<00:05, 225.29it/s]\u001b[A\n",
            "Evaluating:  56%|█████▌    | 1452/2613 [00:07<00:05, 227.21it/s]\u001b[A\n",
            "Evaluating:  56%|█████▋    | 1476/2613 [00:07<00:04, 229.99it/s]\u001b[A\n",
            "Evaluating:  57%|█████▋    | 1500/2613 [00:07<00:04, 225.56it/s]\u001b[A\n",
            "Evaluating:  58%|█████▊    | 1525/2613 [00:07<00:04, 230.86it/s]\u001b[A\n",
            "Evaluating:  59%|█████▉    | 1549/2613 [00:07<00:04, 232.78it/s]\u001b[A\n",
            "Evaluating:  60%|██████    | 1573/2613 [00:07<00:04, 234.72it/s]\u001b[A\n",
            "Evaluating:  61%|██████    | 1597/2613 [00:08<00:04, 235.27it/s]\u001b[A\n",
            "Evaluating:  62%|██████▏   | 1621/2613 [00:08<00:04, 236.22it/s]\u001b[A\n",
            "Evaluating:  63%|██████▎   | 1645/2613 [00:08<00:04, 224.40it/s]\u001b[A\n",
            "Evaluating:  64%|██████▍   | 1670/2613 [00:08<00:04, 230.38it/s]\u001b[A\n",
            "Evaluating:  65%|██████▍   | 1694/2613 [00:08<00:03, 230.70it/s]\u001b[A\n",
            "Evaluating:  66%|██████▌   | 1718/2613 [00:08<00:03, 232.95it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 1742/2613 [00:08<00:03, 230.98it/s]\u001b[A\n",
            "Evaluating:  68%|██████▊   | 1767/2613 [00:08<00:03, 234.22it/s]\u001b[A\n",
            "Evaluating:  69%|██████▊   | 1791/2613 [00:08<00:03, 235.20it/s]\u001b[A\n",
            "Evaluating:  69%|██████▉   | 1815/2613 [00:09<00:03, 235.70it/s]\u001b[A\n",
            "Evaluating:  70%|███████   | 1840/2613 [00:09<00:03, 236.88it/s]\u001b[A\n",
            "Evaluating:  71%|███████▏  | 1864/2613 [00:09<00:03, 227.37it/s]\u001b[A\n",
            "Evaluating:  72%|███████▏  | 1887/2613 [00:09<00:03, 220.35it/s]\u001b[A\n",
            "Evaluating:  73%|███████▎  | 1911/2613 [00:09<00:03, 224.38it/s]\u001b[A\n",
            "Evaluating:  74%|███████▍  | 1936/2613 [00:09<00:02, 230.44it/s]\u001b[A\n",
            "Evaluating:  75%|███████▌  | 1960/2613 [00:09<00:02, 227.73it/s]\u001b[A\n",
            "Evaluating:  76%|███████▌  | 1985/2613 [00:09<00:02, 232.40it/s]\u001b[A\n",
            "Evaluating:  77%|███████▋  | 2009/2613 [00:09<00:02, 233.40it/s]\u001b[A\n",
            "Evaluating:  78%|███████▊  | 2034/2613 [00:09<00:02, 236.93it/s]\u001b[A\n",
            "Evaluating:  79%|███████▉  | 2058/2613 [00:10<00:02, 236.32it/s]\u001b[A\n",
            "Evaluating:  80%|███████▉  | 2082/2613 [00:10<00:02, 237.32it/s]\u001b[A\n",
            "Evaluating:  81%|████████  | 2106/2613 [00:10<00:02, 226.80it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2131/2613 [00:10<00:02, 231.32it/s]\u001b[A\n",
            "Evaluating:  82%|████████▏ | 2155/2613 [00:10<00:01, 231.75it/s]\u001b[A\n",
            "Evaluating:  83%|████████▎ | 2179/2613 [00:10<00:01, 233.88it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 2203/2613 [00:10<00:01, 229.95it/s]\u001b[A\n",
            "Evaluating:  85%|████████▌ | 2227/2613 [00:10<00:01, 231.21it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 2252/2613 [00:10<00:01, 234.97it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 2276/2613 [00:11<00:01, 233.99it/s]\u001b[A\n",
            "Evaluating:  88%|████████▊ | 2301/2613 [00:11<00:01, 236.05it/s]\u001b[A\n",
            "Evaluating:  89%|████████▉ | 2325/2613 [00:11<00:01, 229.59it/s]\u001b[A\n",
            "Evaluating:  90%|████████▉ | 2348/2613 [00:11<00:01, 223.47it/s]\u001b[A\n",
            "Evaluating:  91%|█████████ | 2372/2613 [00:11<00:01, 226.72it/s]\u001b[A\n",
            "Training | Loss: 0.20329102873802185 | Acc: 0.9484536051750183: 100%|█████████▉| 10443/10449 [02:22<00:00, 87.83it/s]\n",
            "Evaluating:  93%|█████████▎| 2418/2613 [00:11<00:00, 227.27it/s]\u001b[A\n",
            "Evaluating:  93%|█████████▎| 2441/2613 [00:11<00:00, 227.10it/s]\u001b[A\n",
            "Evaluating:  94%|█████████▍| 2465/2613 [00:11<00:00, 228.98it/s]\u001b[A\n",
            "Evaluating:  95%|█████████▌| 2490/2613 [00:11<00:00, 233.76it/s]\u001b[A\n",
            "Evaluating:  96%|█████████▌| 2515/2613 [00:12<00:00, 235.94it/s]\u001b[A\n",
            "Evaluating:  97%|█████████▋| 2539/2613 [00:12<00:00, 229.93it/s]\u001b[A\n",
            "Evaluating:  98%|█████████▊| 2563/2613 [00:12<00:00, 214.50it/s]\u001b[A\n",
            "Evaluating:  99%|█████████▉| 2587/2613 [00:12<00:00, 219.98it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 2613/2613 [00:12<00:00, 208.33it/s]\n",
            "Training | Loss: 0.20329102873802185 | Acc: 0.9484536051750183: 100%|██████████| 10449/10449 [02:23<00:00, 72.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2329600885499039\n",
            "Test average accuracy: 0.9375813071681218\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps = np.linspace(1, len(avg_loss), len(avg_loss))\n",
        "#loss = [tensor.detach().numpy() for tensor in avg_loss]\n",
        "#plt.plot(steps, np.array(avg_loss), label=\"Loss\")\n",
        "plt.plot(steps[100:], np.array(avg_acc)[100:], label=\"Accuracy\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(np.array(avg_acc).mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "IbjHshk-a9M4",
        "outputId": "800da14d-955d-4f89-b23e-b0a7920d3a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9394054429629739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "for step, (data_batch, label_batch) in enumerate(tqdm(train_loader, desc=\"Training DuDuDuDu\")):\n",
        "    if not step: print()\n",
        "    print(f\"Batch {step + 1}\")\n",
        "    print(f\"Data batch shape: {data_batch[0].shape} {data_batch[1].shape}\")  # Should be (16, 10) for batch size 16 and 10 features\n",
        "    print(f\"Label batch shape: {label_batch.shape}\")  # Should be (16,) for batch size 16\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1Ch1rN0HK4t",
        "outputId": "5c9359ed-635f-43ba-95ec-a970b17472f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DuDuDuDu: 100%|██████████| 4/4 [00:00<00:00, 276.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 1\n",
            "Data batch shape: torch.Size([2, 44]) torch.Size([2])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n",
            "Batch 2\n",
            "Data batch shape: torch.Size([2, 44]) torch.Size([2])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n",
            "Batch 3\n",
            "Data batch shape: torch.Size([2, 44]) torch.Size([2])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n",
            "Batch 4\n",
            "Data batch shape: torch.Size([2, 44]) torch.Size([2])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define a simple custom dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # Generate some example data: 100 samples, each with 10 features\n",
        "        self.data = torch.randn(10, 3)  # 100 samples, 10 features per sample\n",
        "        self.lengths = torch.randint(1, 10, (10,))\n",
        "        self.labels = torch.randint(0, 2, (10,))  # Binary labels (0 or 1) for each sample\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total number of samples\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data sample and its corresponding label\n",
        "        sample = [self.data[idx], self.lengths[idx]]\n",
        "        label = self.labels[idx]\n",
        "        return sample, label\n",
        "\n",
        "# Create an instance of the dataset\n",
        "dataset = MyDataset()\n"
      ],
      "metadata": {
        "id": "9jVb8YexxEh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Create a DataLoader with a batch size of 16\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "print(dataloader)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "\n",
        "for step, (data_batch, label_batch) in enumerate(tqdm(dataloader, desc=\"Training DuDuDuDu\")):\n",
        "    if not step: print()\n",
        "    print(f\"Batch {step + 1}\")\n",
        "    print(f\"Data batch shape: {data_batch[0].shape} {data_batch[1]}\")  # Should be (16, 10) for batch size 16 and 10 features\n",
        "    print(f\"Label batch shape: {label_batch.shape}\")  # Should be (16,) for batch size 16\n",
        "    print(\"-\" * 30)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMSmB3jKydWJ",
        "outputId": "537853a4-c46f-4619-b70d-8fdc466d2bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7e839b7face0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DuDuDuDu: 100%|██████████| 5/5 [00:00<00:00, 419.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 1\n",
            "Data batch shape: torch.Size([2, 3]) tensor([2, 4])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n",
            "Batch 2\n",
            "Data batch shape: torch.Size([2, 3]) tensor([6, 5])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n",
            "Batch 3\n",
            "Data batch shape: torch.Size([2, 3]) tensor([8, 1])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n",
            "Batch 4\n",
            "Data batch shape: torch.Size([2, 3]) tensor([1, 8])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n",
            "Batch 5\n",
            "Data batch shape: torch.Size([2, 3]) tensor([6, 6])\n",
            "Label batch shape: torch.Size([2])\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(torch.utils.data.DataLoader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0i3cgQ5xEo_",
        "outputId": "9f2e0a2c-4e03-4aa2-854e-bcd57947c02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class DataLoader in module torch.utils.data.dataloader:\n",
            "\n",
            "class DataLoader(typing.Generic)\n",
            " |  DataLoader(dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int] = None, persistent_workers: bool = False, pin_memory_device: str = '')\n",
            " |  \n",
            " |  Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
            " |  \n",
            " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
            " |  iterable-style datasets with single- or multi-process loading, customizing\n",
            " |  loading order and optional automatic batching (collation) and memory pinning.\n",
            " |  \n",
            " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
            " |  \n",
            " |  Args:\n",
            " |      dataset (Dataset): dataset from which to load the data.\n",
            " |      batch_size (int, optional): how many samples per batch to load\n",
            " |          (default: ``1``).\n",
            " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
            " |          at every epoch (default: ``False``).\n",
            " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
            " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
            " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
            " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
            " |          returns a batch of indices at a time. Mutually exclusive with\n",
            " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
            " |          and :attr:`drop_last`.\n",
            " |      num_workers (int, optional): how many subprocesses to use for data\n",
            " |          loading. ``0`` means that the data will be loaded in the main process.\n",
            " |          (default: ``0``)\n",
            " |      collate_fn (Callable, optional): merges a list of samples to form a\n",
            " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
            " |          map-style dataset.\n",
            " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
            " |          into device/CUDA pinned memory before returning them.  If your data elements\n",
            " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
            " |          see the example below.\n",
            " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
            " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
            " |          the size of dataset is not divisible by the batch size, then the last batch\n",
            " |          will be smaller. (default: ``False``)\n",
            " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
            " |          from workers. Should always be non-negative. (default: ``0``)\n",
            " |      worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
            " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
            " |          input, after seeding and before data loading. (default: ``None``)\n",
            " |      multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n",
            " |          ``None``, the default `multiprocessing context`_ of your operating system will\n",
            " |          be used. (default: ``None``)\n",
            " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
            " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
            " |          ``base_seed`` for workers. (default: ``None``)\n",
            " |      prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
            " |          in advance by each worker. ``2`` means there will be a total of\n",
            " |          2 * num_workers batches prefetched across all workers. (default value depends\n",
            " |          on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
            " |          Otherwise, if value of ``num_workers > 0`` default is ``2``).\n",
            " |      persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n",
            " |          the worker processes after a dataset has been consumed once. This allows to\n",
            " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
            " |      pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n",
            " |          ``True``.\n",
            " |  \n",
            " |  \n",
            " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
            " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
            " |               :ref:`multiprocessing-best-practices` on more details related\n",
            " |               to multiprocessing in PyTorch.\n",
            " |  \n",
            " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
            " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
            " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
            " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
            " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
            " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
            " |               loading to avoid duplicate data.\n",
            " |  \n",
            " |               However, if sharding results in multiple workers having incomplete last batches,\n",
            " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
            " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
            " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
            " |               cases in general.\n",
            " |  \n",
            " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
            " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
            " |               `Multi-process data loading`_.\n",
            " |  \n",
            " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
            " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
            " |  \n",
            " |  .. _multiprocessing context:\n",
            " |      https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataLoader\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int] = None, persistent_workers: bool = False, pin_memory_device: str = '')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
            " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
            " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |  \n",
            " |  __setattr__(self, attr, val)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  check_worker_number_rationality(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  multiprocessing_context\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_iterator': typing.Optional[ForwardRef('_BaseDataL...\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  __parameters__ = (+T_co,)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
